[[{"l":"Welcome","p":["Welcome to the official DataWars' handbook for the Delivery team. It should be able to answer ALL your questions, and help you start on any position.","This is a work in progress. You are welcome and encouraged to collaborate and send Pull Requests to the main repo. If you find any problems, create an issue.","Here are a few quick links:","Are you a first time author starting our training? Go to first time authors docs."]}],[{"l":"Understanding DataWars","p":["DataWars Mission","The DataWars model","What makes a good project?","Structure of a project","What makes a good Dataset?","Examples of great projects"]}],[{"l":"DataWars Mission","p":["DataWars mission is to complement the abundant learning material that is out there (youtube videos, books, blogs, free courses, etc) with real life projects.","Our type of customer is someone that wants to learn by doing and build experience with Data Science. They work/study full time. This is going to be important by the time we discuss about what makes a good project."]}],[{"l":"DataWars Work Policy","p":["All employees must acknowledge they have read, understood, and agree to adhere to this policy.","All team members are expected to attend scheduled meetings, including daily scrums and other necessary team calls.","Any blockers or issues requiring assistance","Each author must post a daily update by the end of their workday. This update should include:","Each employee is accountable for their own work and must ensure that they meet deadlines and quality standards.","Employees must utilize the company-provided tools and resources for communication, project management, and collaboration. Ensure proper use of these tools to maintain productivity and efficiency.","Employees should respond to messages and emails from team members and stakeholders within a reasonable timeframe, preferably within 24 hours.","Failure to comply with this policy may result in performance reviews, and repeated non-compliance could lead to further disciplinary actions.","for daily updates (Slack)","If a team member is unable to attend a meeting, they must inform their team in advance and provide a brief update on their progress and plans on the same chanel for daily updates (Slack)","Regular check-ins with managers or team leads to discuss progress and address any issues are encouraged","Respect and support colleagues, fostering a positive and inclusive remote work environment.","Tasks completed","Tasks planned for the next day","This policy is designed to provide clear guidelines for remote work, ensuring that all team members, regardless of location, can work effectively and collaboratively. It aims to balance flexibility with accountability and maintain high standards of communication and responsiveness.","This policy will be reviewed regularly and updated as necessary to adapt to changing needs and circumstances.","Updates should be posted in the designated communication channel","While we do not enforce standard office hours, team members should overlap at least 4-5 hours with the core team working hours to facilitate meetings and collaboration.Considering everyone’s time zone and style of working - Let us be actively available for each other between 5:00 PM - 10:00 PM IST"]}],[{"l":"DataWars Leave Policy"},{"l":"Purpose","p":["This Paid Time Off (PTO) Policy outlines the guidelines for taking time off for various personal, health, and study reasons. The policy aims to provide employees with flexibility and ensure they maintain a healthy work-life balance."]},{"l":"Scope","p":["This policy applies to every full time, part time and contractual employee regardless of their position and department in the company"]},{"l":"Guidelines","p":["Employees are eligible for unlimited paid time off for health issues, including physical and mental health, and study purposes.","For health-related PTO, employees must notify their manager longer than 3 consecutive days. This will help to manage workload effectively. And, for unexpected illnesses, please keep your manager informed as soon as possible.","For study-related PTO, employees should submit their request at least 8 weeks in advance.","It is important to make sure that the team has enough resources to complete any pending tasks and keep the work going before implementing an unlimited PTO policy.","Misuse of the PTO policy will be addressed with appropriate disciplinary action, up to and including termination of employment","The Employee will have 12 Annual Leave, 5 Casual Leave in a year and can carry 5 numbers of leaves next year. The carried leaves can be encashed during their full and final settlement period.","Public Holidays are officially observed in India and Argentina. Any other country will be treated individually.","Employees shall submit the leave request here","Here are the Public Holiday List for 2024 - India& Argentina"]}],[{"l":"The datawars model","p":["A in DataWars is always enclosed in , which in turn is contained in a . Multiple are organized within a based on their and .","A denotes a single, atomic Skill that a user \"has\". For example, \"Dealing with null values\", or \"Dealing with duplicate values\". These are then logically aggregated in a . In this case, it'll be \"Data Cleaning\".","are subjective aggregations that we have decided to create.","A defines a \"role\" for a user, divided in multiple Learning Areas and Proficiencies. Within the juncture of Proficiency and Learning Area, you'll find a ."]}],[{"i":"what-makes-a-good-project","l":"What makes a good project?","p":["Understand the user's perspective an point of view","Starts slowly and progresses with the user, helping them navigate the complexity of the skill and learning about the dataset/challenge they're working with.","Starting slow will provide small dopamine injections that will keep the user engaged.","This will translate into a \"Flow\" state."]}],[{"l":"Structure of a project"}],[{"i":"what-makes-a-good-dataset","l":"What makes a good Dataset?","p":["A good dataset is 90% of the impact of a good project. A dataset that is interesting, engaging, of good quality is what keeps your students engaged.","Alignment with Project Goals: How well does the data address the specific research question or project objective?","Also, it’s very useful when we combine data from different sources to create a richer dataset. For example:","Complexity: have good relationships among variables/features.","Data Relevance:","Domain of work: for example, if you’re a biologist and know about some particular type of dataset that is publicly available for biology but nobody knows about it, it’ll make it more interesting.** Spark curiosity and potential","Fun, interesting, engaging: a dataset that teaches you something new aside from the topic you’re working on. For example, if you’re building a ML project, and using a DataSet about the impact of Nitrogen in Maize production, is something “unusual” and that most people will find interesting. Things about hobbies (music, movies, video games), engineering, science, biology will make things more interesting.","Go to the Australian Open Data portal and find the Electricity production of each power plant in the country. Then group per city to find total electricity per city. Then, find statistics about cities in the country (like population, income, weather, etc). Finally combine everything to make a dataset: Statistics of Australian cities 1990-2023.CSV","Go to the World Bank data site and find the production of rice per country. Then combine it with rainfall for all the countries. Finally, combine it with average temperatures. This would be AMAZING dataset: Rice production and weather data in the world.CSV","Good quality: Does it have enough rows and columns? Is there variety or all the columns are the same? This collaborates with the broader picture of the project.","Interesting stories (For exp, Cure the princes dataset or Livigno weather dataset, as it tells some stories behind the data)","Local/close to you: If you find datasets that are somehow closer to you, it’ll help you connect with your students. For example, I’m from Argentina and I found a dataset with all the Baby names used in Argentina.","Novelty: Data explores new ideas, events and areas","The key we want to focus on is the “qualitative properties” of the dataset:","The type of dataset (CSV, JSON, binary), the properties and features depends on the type of project you’re creating, so we won’t get into those details.","Topic Coverage: Does the data capture the key aspects and subtopics relevant to the chosen theme?"]},{"i":"where-can-you-get-datasets-from-ideas-of-public-websites","l":"Where can you get datasets from? Ideas of public websites","p":["Academic Publications and Research Papers: Reviewing relevant academic papers to identify potential data sources used by researchers in a particular field.","CDC (health US) Data Catalog: https://data.cdc.gov/browse","Data Science Blogs and Communities: Follow data science blogs, participate in online forums, and connect with other data scientists. This allows you to stay updated on new data repositories, emerging trends, and potential data sources shared by the community","data-is-plural.com: amazing weekly newsletter with access to a lot of datasets curated and explained","https://databank.illinois.edu/","https://dhsprogram.com/","https://ieee-dataport.org/- Interesting technical datasets. Requires a free account.","https://paperswithcode.com/- These are usually already replicated in Kaggle, but there are exceptions from time to time","https://ukdataservice.ac.uk/( https://internationaldata.ukdataservice.ac.uk/, https://stats2.digitalresources.jisc.ac.uk/)","https://www.pangaea.de/- Environmental datasets, a lot of time series and interesting climatic/chemical/biological datasets","https://www.societyforscience.org/research-at-home/large-data-sets/","https://zenodo.org/search- Mostly european and biological / climate, has good datasets.","Industry News and Publications: Stay informed about industry news and publications related to your domain. Announcements about new data initiatives or collaborations might reveal fresh data sources","Industry Reports and Datasets: Industry reports, whitepapers, or data published by companies or organizations related to the project theme. These might offer unique insights and datasets not readily available elsewhere.","It’s rather easy to find original and interesting datasets on the internet. We have done it for more than 100 projects already. We’re asking for your help because we’re growing and need the extra help. Here’s a list of places we usually get datasets from:","Keyword Research - Dataset based on list of keywords related to a particular subject/theme, such as (Agriculture - Crop dataset, soil health dataset or crop disease dataset, etc. For example, USDA NASS data portal)","Maven Analytic challenges: https://mavenanalytics.io/challenges","Niche data portals - things that are specific to your domain. For example, a cyber security data website, or the climatic data from a region in Italy.","Others channels where we can find data sources:","Recent Events and Activities: Utilize social media listening tools to identify relevant conversations and discussions happening online. Recent events like the country election[US Election], pandemic [Covid Dataset], etc. This can lead you to new data sources or user-generated content related to your topic","StackExchange& Reddit /r/datasets- Most of the time the datasets are already replicated to (or copied from) Kaggle. But there are some good gems from time to time.","Subject-Specific Repositories - these repositories are specific to our project's domain. For example, research data repositories might be available for healthcare, finance, or education. These can offer targeted datasets and resources relevant to our research question.","There’s also an API and a repo","UC Berkeley data: https://dlab.berkeley.edu/data/uc-data","US National Park Service data: huge catalog of data associated with US National Parks","World Bank Data- most datasets are uninteresting, but there are some gems as well","Your country’s data portal - this is ideal to find things that are unique to you. For example, the UK data portal, Ireland’s data portal, or the Argentina data portal. There’s a prevalent list here."]},{"l":"Dataset Quality Framework","p":["Metrics for evaluating dataset quality. These metrics can be implemented through a scoring system or a checklist. A point system for each metric, with higher scores indicating better quality. Alternatively, a checklist can ensure all essential criteria are met for a \"good\" dataset.","Quality assurance and progress monitoring is implemented using spreadsheets.","Volume and variety: Dataset should contains enough rows (~1000), features and data points","Accuracy and cleanliness: The dataset should be reliable and free from errors(no inconsistency and unnecessary columns or rows)","Completeness: Dataset with minimal missing values and outliers is preferred ( more missing values introduce biases and hinder analysis)","Source Credibility: Evaluation of the data source's reputation and reliability. ( government portal generally have high credibility than public data portals and sources)","Documentation Clarity: How well does the accompanying documentation explain the data structure, variables, and usage?"]}],[{"l":"Understanding the scope of a project","p":["It's VERY important to understand the scope of the project you're working with (given by the Skill)."]}],[{"l":"Examples of great projects"}],[{"l":"Content creation process","p":["Must link to Good Practices for authors to strictly follow the guidelines."]}],[{"l":"Project testing","p":["A step by step list of things to do and follow when testing a project.","Testing Projects","Old video:"]}],[{"l":"Datasets doc"},{"i":"step-1-finding-datasets","l":"Step 1: Finding datasets"},{"i":"dataset-can-be-found-on-various-data-sources-here-are-the-ways-for-finding-datasources-searce-for-repository-with-relevant-keyword-there-are-public-and-government-data-repository-that-provide-data-of-different-domain","l":"Dataset can be found on various data sources. Here are the ways for finding datasources. Searce for repository with relevant keyword. There are public and government data repository that provide data of different domain.","p":["For exp: GBIF Global Biodiversity Information Facility—is an international network and data infrastructure aimed at providing anyone, anywhere, open access to data about biodiversity and all types of life on Earth.","Academic Publications and Research Papers: Reviewing relevant academic papers to identify potential data sources used by researchers in a particular field.","Industry Reports and Datasets: Industry reports, whitepapers, or data published by companies or organizations related to the project theme. These might offer unique insights and datasets not readily available elsewhere.","Recent Events and Activities: Utilize social media listening tools to identify relevant conversations and discussions happening online. Recent events like the country election[US Election], pandemic [Covid Dataset], etc. This can lead you to new data sources or user-generated content related to your topic.","Data Science Blogs and Communities: Follow data science blogs, participate in online forums, and connect with other data scientists. This allows you to stay updated on new data repositories, emerging trends, and potential data sources shared by the community","Industry News and Publications: Stay informed about industry news and publications related to your domain. Announcements about new data initiatives or collaborations might reveal fresh data sources"]},{"i":"step-2-extracting-and-processing-datasets","l":"Step 2: Extracting and processing datasets","p":["Dataset can be downloaded from the source, in suitable format avilable like CSV, JSON, XML, etc. In next step, data files can be transformed into structured format such as CSV."]},{"i":"step-3-writing-dataset-description","l":"Step 3: Writing dataset description","p":["Add images to make the description more interactive","Application of dataset: Mention the possible application domains of the dataset. This helps in understanding the dataset better. Applications of dataset","Data path: Mention the path of the dataset. This is important as it helps in locating the dataset easily.","Database path is: {{devices.\"Data Source\".ip_address}}","Dataset path is: {{datasource.filesystem_path}}/\"filename\"","Dataset use example: Mention the possible use case of the dataset. Use cases","df.head() Adding preview of the dataset gives a brief idea about the dataset. Dataset Preview","df.info(): This method prints information about a DataFrame including the index dtype and column dtypes, non-null values, and memory usage. Info of Dataset","For data files with CSV file format, the path can be mentioned as follows: {{datasource.filesystem_path}}/\"filename\"","for data files with csv format: WHO Malaria Report 2023","For databases, the ip address path can be written as follows: {{devices_copy.\"Data Source\".ip_address}} Dataset path","for databases: Employees Database","For Refrence to dataset description, here are some of the description file:","Introduction: Briefly introduce the dataset and the source from where it was obtained. Adding fact and figures about the dataset makes it look more interesting. Describe how the data was collected. This could be through surveys, experiments, observations, or other methods. Mention the source of the dataset. This could be a link to the original dataset or the name of the organization that provided the data.","Note:","Recommended Usage","Recommended use in Python or R language: Mention the recommended usage of the dataset in Python or R language. This provides code to load file and preview.","Things to consider while writing dataset description:","To access database tables in Python, use the following code:","To read data file in python use the following code:","Update frequency: Mention the frequency of the dataset update. This helps in understanding the dataset better. Update Frequency","Use H3 markdown format for writing headings","Use markdown format for writing dataset description"]},{"i":"step-4-uploading-datasets-to-repositoy","l":"Step 4: Uploading datasets to repositoy","p":["Dataset file of CSV format had to be uploaded to github repository Github Repository Dataset Repository","Dataset files must be stored in dataset folder only.","Configuration of datafiles must be done in config.csv file.","Syntax for configuration of datafiles in config.csv file: Adding Dataset to Repository","Syntax:","The dataset must be uploaded to the repository in the correct format and structure. This ensures that the dataset is easily accessible."]}],[{"l":"New authors training","p":["You will be working with a Github repository for your project. Let's understand how it works. This guide will help you understand the project structure, the artifacts in the repository, and how to transform the project, everything you need to know in order to start working on your project."]}],[{"l":"Your first project","p":["Getting Started with DataWars","What is an activity?","Activity Types","Choosing the dataset","Example notebook activities","Good Practices to follow by Authors"]}],[{"l":"Getting started","p":["It's important to understand the structure of a project first. The project is divided into two main sections:"]},{"l":"Left pane","p":["Contains the \"content\" of the project, including whatever the student has to read + the activities.","In the left pane, you can include activities, that are automatically graded. There are different activity types. Check What is an activity? and Activity Types for more."]},{"l":"Right pane","p":["Includes the \"interactive lab\", which is whatever is \"executable\" by the student. This should have almost NO content."]},{"l":"Next on","p":["Check the different What is an activity? and Activity Types available or how to create your first project from a Notebook."]}],[{"l":"What is an activity","p":["DataWars' projects are all about creating interesting and engaging activities. An activity in general contains:","Title: a short, descriptive, ideally imperative description of the objectives of the activity","Description: A free text (unlimited) description to help the student achieve the objectives of the activity, can include images or videos.","Solution(optional, strongly recommended): A solution that will be revealed upon request. Solutions can contain anything: text, code, images, GIFs, videos, etc. Although it's optional, MOST activities SHOULD contain a solution.","Hints(optional): A series of hints that can help the user. Also can contain any type of text, images, etc.","Expected outcome(optional): An example of what the outcome of the activity should be. It can be a screenshot of the dataframe, a table with the SQL results, an example of executing a function, etc.","There are different types of activities (read more in Activity Types):","Multiple choice","Input","Code/interactive with and without user input"]}],[{"l":"Activity types","p":["Here are the different activity types you can include in your project:"]},{"l":"Input activities","p":["Input activities give the user a blank field to enter their answer. It can be a numeric answer, or a text answer. For example:","How many null values are in the column X?: The answer is numeric, integer 250","What's the mean of the column X? The answer is numeric, but a float: 25.80.","What's the most common city in the dataset? The answer is a string New York City.","Float and string values are checked EXACTLY as they're provided. Strings are CASE SENSITIVE. And for floats, read the note below:","If your input activity's correct answer is a float, make sure you explain the user how many decimal values you're considering as valid, providing examples. Example:"]},{"l":"Multiple choice activities","p":["Multiple choice activities are as simple as they get. They can be configured to have one or more correct values (the widget rendered will be a radio button or a checkbox, respectively)."]},{"i":"code-activities-without-input","l":"Code activities (without input)","p":["\"Code\" activities are designed to check something from the user interactively. They're advanced types of activities and apply to MORE than just Jupyter activities. Internally, we can run custom code to check that the user's code passes the grading provided."]},{"i":"code-activities-with-input","l":"Code activities (with input)","p":["Similar to a regular code activity, the user has to provide some input that is evaluated dynamically."]}],[{"l":"How to create your first project","p":["Creating your first project is extremely simple. It all starts with a single Jupyter notebook. The general structure of your notebooks should be:","And here's the link to the notebook: Sample Project.ipynb"]}],[{"l":"Choosing a dataset","p":["Projects can be configured to use a DataWars Playgrounds data source automatically. Here's a step by step guide to do so.","In this document","How to use Playground Datasets in your projects","Examples of projects using datasources"]},{"l":"How to use Playground Datasets in your projects"},{"l":"1. Find the data source to use","p":["Data Sources can be either file-based datasets (a CSV, a directory containing image files) or a \"device\", for example, a MySQL Database.","Let's try to add both.","Go to the Datasets section of Playgrounds and find the datasets you want to add. In this case I'll add:","Pagila: a PostgreSQL database containing movie rentals","Hollywood Movies Database: a CSV file containing hollywood movies data","What you'll need to do is:","Go to datasets","Find the desired dataset (searching or using the filters on the right)","Hover over the logo and you'll see the ID","Copy the ID of the data source using the button","In this case, here are the IDs:","Pagila: 7742f868-902a-4262-bfbe-00497ac27468","Hollywood Movies Database: de5011a1-61fb-45f2-b067-bd363ba012d2"]},{"i":"2-configure-your-projects-docker-composeyml","l":"2. Configure your Project's docker-compose.yml","p":["Now you have to define in your project's docker-compose.yml. There's a new section at the end of the YAML file that is called x-datasources:","You can list as many datasources as you want. If the datasource is a \"device\" type (like the Postgres Pagila one), it'll be automatically added to your project. But, if your dataset is a \"file type\" (a CSV, a Directory), you also have to configure it in the service that will have access to those files.","Here's a working example with comments:"]},{"l":"3. Finding the data in the lab","p":["If the data source is a \"device type\", it'll be in the final lab once you import it, easy.","If the data source is of file type, it'll be mounted in the /data directory."]},{"l":"Examples of working projects"},{"i":"python--pagila-postgres","l":"Python + Pagila (postgres)","p":["This project ( Practice accessing Postres from Python using Pagila) has two devices:","Here's its docker-compose.yml:"]}],[{"i":"#","p":["Guidelines to create engaging and effective projects."]},{"l":"Good Practices for Project Authoring","p":["When designing a project, it’s crucial to keep two primary goals in mind:","Avoiding Boredom","Preventing Exhaustion","The following guidelines will help you meet these goals effectively."]},{"l":"1. Define and Manage Project Scope","p":["A well-defined scope is essential to ensure users face challenges that match their skill level and progress gradually. Understanding what the users already know and what they’re expected to learn is key.","For example, if you're creating a project within the Data Cleaning with Pandas skill track, asking users to perform a simple task like creating a Series from a column in a DataFrame may lead to boredom as it lacks sufficient challenge. Conversely, asking them to perform complex operations like merging datasets before covering the necessary concepts may overwhelm them. Ensure each project aligns with the skills already covered to keep users engaged without frustration."]},{"l":"2. Format Titles and Descriptions Thoughtfully","p":["Activity Titles: Always frame titles as questions to engage the user. For example, instead of “Capital of Argentina,” use “What is the Capital of Argentina?” This sparks curiosity and primes the user for problem-solving.","Activity Descriptions: Ensure that descriptions are clear, concise, and well-structured. Avoid long blocks of text, as they can overwhelm users, especially those juggling full-time work or study alongside their learning. Most users are looking to learn Data Science through hands-on coding exercises, so lengthy descriptions can exhaust them and make the project less enjoyable.","Use bold or italics to emphasize key points.","Enclose code elements (e.g., variable or column names) in backticks (`) to distinguish them from text.","Use triple backticks (```) for code blocks to improve readability.","If you can’t avoid lengthy descriptions, such as in capstone projects, try breaking the task into smaller activities. If that’s not feasible, use tools like ChatGPT to improve clarity and structure.","Example Prompt: You are a data science professor. I need your help improving the formatting and clarity of my activity descriptions. I'll provide the title and description, and you will revise them.","Example: Notice how ChatGPT transformed the activity title and description.","Before: Activity Before Formatting","After: Activity After Formatting","The Activities Should Be Atomic: One major drawback of longer activity descriptions is that they often lead to multiple steps within a single task. This complexity makes it harder to test and pinpoint errors. If an activity fails, users may not know where they made a mistake. For example, consider the following activity that requires users to complete four steps: calculating common_chatbots, defining the fill_chatbot function, applying this function to the chatbot_name column to impute missing values, and filling any remaining missing values with the string Unknown. If the task fails, users might struggle to identify the specific step where they made an error.","Not an Atomic Activity"]},{"l":"3. Use Placeholders in Jupyter Notebook","p":["Provide placeholders whenever users are asked to complete an activity. For example, if they need to store results in a variable or create a new column in a DataFrame, placeholders help guide them. This minimizes confusion and helps keep the task focused.","Examples of placeholders: Variable Placeholder Column Placeholder Function Placeholder"]},{"l":"4. Provide a Preview of Expected Outcomes","p":["Where possible, include a preview of the expected result. For example, in a visualization project, show the desired plot, or when manipulating a DataFrame, provide a snapshot of the expected output. These visual or textual cues help users understand what the end goal looks like, reducing frustration and ensuring they are on the right path.","Examples: Expected Outcome DataFrame Expected Outcome Visualization"]},{"l":"5. Practice Empathy","p":["Think from the user’s perspective and ask:","What might confuse or challenge the user at this stage?","Are the instructions clear for someone with the expected level of knowledge?","Would I feel confident completing this task, given what I’ve learned so far?","Empathy-driven design leads to a smoother learning experience by anticipating user needs.","Example: Example Activity","Consider the activity in the screenshot above where users are asked to input answers. If they can't solve it and request a solution, they receive the correct value to pass. But pause and think—what was on their minds?","The users requested help because:","They wrote code but got the wrong result, or","They didn’t know how to approach the problem.","In both cases, just giving the answers isn’t enough. They expect to learn how to solve the problem. The solution should include code or explanations demonstrating the correct process, reinforcing their learning for future tasks."]},{"l":"6. Anticipate and Address Edge Cases","p":["Be mindful of edge cases and provide detailed instructions to address them. Whenever an edge case arises, clearly outline the steps users should follow to handle it. This prevents users from feeling stuck or confused when encountering unexpected situations.","For example: Look how different ways of calculating frequency counts result in different order of index when their values are equal. In this case, we should mention in the activity description that the user should use value_counts().","Edge Case Example"]},{"l":"7. Manage Time Expectations and Feedback","p":["Consider the time required to complete the project. For Learn and Practice projects, aim for 25–30 minutes, while for Assignments and Capstone Projects, a duration of 45–60 minutes is ideal. Managing time expectations ensures the project remains engaging and manageable without overwhelming users.","By following these best practices, you’ll create projects that are engaging, appropriately challenging, and enjoyable for learners. This balance will help keep users motivated and make their learning experience both rewarding and frustration-free."]}],[{"l":"Assertion Framework Documentation","p":["In this assertion framework documentation, you'll learn how to use already defined function to check/assert the activity.","This assertion framework documentation is divided into two parts, first part cover the most used assertion function with example activities which teach you how to save the expected result and then check it with users/student solution. Second part cover the not only how to use them but in more details like what are the parameters, what are the return types, etc.","Make to use the same versions of required libraries as used in the platform. You can check the version of the library by running the following command in the notebook cell:","Similarly, you can check the version of other libraries like numpy, matplotlib, etc.","Functions Assertion","Python List Assertion","Python Tuple Assertion","Python Dictionary Assertion","Python Sets Assertion","Pandas Series Assertion","Pandas DataFrame Assertion","SQL Assertion","Matplotlib Assertion","PyTorch Assertion","Numbers Assertion"]}],[{"l":"Function Assertions by Examples","p":["In this notebook, you'll learn how to use most used Functions checking assertion functions. Below are the functions that I've covered in this notebook.","assert_student_function_name_equals(student_function_name, fn_args=None, fn_kwargs=None, expected_value=DataWarsConstants.EMPTY): Checks that the student's function named student_function_name returns the expected value.","assert_student_function_test_cases(student_function_name, test_cases): Checks that the student's function named student_function_name returns the expected value for each test case in the list test_cases.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions.","Create a functions add() that takes two arguments and returns the sum of the two arguments.","Solution:","Out expected function should return the sum of the two arguments. So, we use assert_student_function_name_equals() function to assert the solution with the student function.","Assertions:","Define a function define_movies that takes a dictionary, id, name, year, and rank as arguments and returns the dictionary with the movie details. Function also takes empty dictionary as first argument which will be used to store the movie details.","Here we have two expected outputs, so we save them in two different dictionaries and then use assert_student_function_test_cases() function to assert the solution with the student function. Assertions:","Define a function define_roles that takes a dictionary, actor_id, and role as arguments and returns the dictionary with the actor's roles. Function also takes empty dictionary as first argument which will be used to store the actor's roles.","Here we have two expected outputs, so we save them in two different dictionaries and then use assert_student_function_test_cases() function to assert the solution with the student function."]}],[{"l":"List Assertions by Examples","p":["In this notebook, you'll learn how to use most used List assertion functions. Below are these functions:","assert_list_variable_equals_variable(student_variable_name, expected_variable_name, delete_afterwards=True): Checks that the student's list in student_variable_name matches the list in expected_variable_name.","assert_list_variable_equals_json(student_variable_name, json_file_name): Checks that the student's list in student_variable_name matches the list contained in the solution JSON file named json_file_name.","assert_list_variable_equals_pickle(student_variable_name, pickle_file_name): Checks that the student's list in student_variable_name matches the list contained in the solution pickle file named pickle_file_name.","Load the utils.py file to use the assertion functions."]},{"l":"Student Data for Activities","p":["In the activities, we'll use the below student data. Format of the data is name, marks, age, grade, subject.","There are total 500 students in the list, but we are showing only a few here."]},{"l":"Activities","p":["As the expected output is a list of integers containing total of 500 students and it is a large list, so we use assert_list_variable_equals_json() function to assert the solution with the student list.","As the expected output is a list of tuples which is small, so we use assert_list_variable_equals_pickle() function to assert the solution with the student list as JSON can't save tuples.","As we can see the expected output is a list of list which is small(only 6 students), so we use assert_list_variable_equals_variable() function to assert the solution with the student list.","Assertion:","Assertions:","Convert the ages into int as currently they are type str(string).","Create a list select_student from the original list( student_list), starting at index 98 up to, but not including, index 104.","Create two lists a_graders and b_graders with the name, age, grade, and favorite subject of the students. Put students with A grades to the list a_graders and students with B graders to the list b_graders","Expected output for a_graders:","Expected output:","Given the list student_list is a list of lists. Convert it into a list of tuples and store it in the variable student_list_tuple.","Here, we have two expected outputs, so we save them in two different json files and then use assert_list_variable_equals_json() function to assert the solution with the student list.","Loop through the list student_list, access the age, cast it to int, and append it to the new list variable students_age.","Make sure to not change the order of the students.","Solution:","Solutions:","This is how we can save the expected output in a json file:","This is how we can save the expected output in a pickle file:","This is just a simple conversion from list of lists to list of tuples. Expected output is different from above example list of tuples.","We use assert_list_variable_equals_json() function to assert the solution with the student list as the expected output is a list of list which is is quite large."]}],[{"l":"Tuple Assertions by Examples","p":["In this notebook, you'll learn how to use most used Tuple assertion functions. Below are these functions:","assert_tuple_variable_equals_variable(student_variable_name, expected_variable_name, delete_afterwards=True): Checks if student tuple variable is equals to expected tuple variable.","assert_tuple_variable_equals_pickle(student_variable_name, pickle_file_name): Checks if student tuple variable is equals to expected pickle file.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Below are two tuples having students records of two cohorts for Computer Science course this semester, each cohort has 30 students","Each tuple is as (student_name, overall_grade).","Combine the records of both cohorts into one tuple named combined.","Solution:","As the expected tuple is quite large, we can save it in a pickle file and then use assert_tuple_variable_equals_pickle() function to assert the solution with the student tuple.","This is how you can save the tuple in a pickle file.","Assertions:","First, find the highest grade and then find the student with that grade then store the details in a tuple named top_student.","As the expected tuple only contains one student, we can easily compare it with the student tuple, we can use assert_tuple_variable_equals_variable() function to assert the solution with the student tuple."]}],[{"l":"Dict Assertions by Examples","p":["In this you'll learn how to use most used Dictionary assertion functions. Below are the functions that are covered.","assert_dict_variable_equals_variable(student_variable_name, expected_variable_name, delete_afterwards=True): Checks that the student's dictionary in student_variable_name matches the dictionary in expected_variable_name.","assert_dict_variable_equals_json(student_variable_name, json_file_name): Checks that the student's dictionary in student_variable_name matches the dictionary contained in the solution JSON file named json_file_name.","assert_dict_variable_equals_pickle(student_variable_name, pickle_file_name): Checks that the student's dictionary in student_variable_name matches the dictionary contained in the solution pickle file named pickle_file_name.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions.","We'll use the below fruits data for the activities.","Use below data to create a dictionary named student_dict.","Expected Output:","Solution:","As the expected output is small dictionary, so we use assert_dict_variable_equals_variable() function to assert the solution with the student dictionary.","Assertions:","Use the fruits_data list to create a dictionary named fruits where the key is the fruit name and the value is a dictionary containing the color and price of the fruit.","In the previous activity, we used assert_dict_variable_equals_variable() function to assert the solution with the student dictionary. But here the expected output is large, so we use assert_dict_variable_equals_json() function to assert the solution with the student dictionary.","Store the result in a variable red_fruits.","Store the result in a variable red_blue_fruits, here the key is the color and value is the tuple containing two values - color and its price.","As we know that when we save tuple in json file, it will be saved as list. So, we use pickle file to save the expected output and then use assert_dict_variable_equals_pickle() function to assert the solution with the student dictionary."]}],[{"l":"Sets Assertions by Examples","p":["In this notebook, you'll learn how to use most used Sets assertion functions. Below are these functions:","assert_set_variable_equals_variable(student_variable_name, expected_variable_name, delete_afterwards=True): Checks if student set variable is equals to expected set variable.","assert_set_variable_equals_json(student_variable_name, json_file_name): Checks if student set variable is equals to expected json file.","assert_set_variable_equals_pickle(student_variable_name, pickle_file_name): Checks if student set variable is equals to expected pickle file.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions.","You have two sets of students enrolled in two different courses. The sets are as follows:","Find the common students enrolled in both courses.","Solution:","As the expected set is small and have only few elements, we can easily compare it with the student set, we can use assert_set_variable_equals_variable() function to assert the solution with the student set. Assertions:"]},{"l":"Activities on Badge Data","p":["For other assertions examples, we will use the badge data. The badge data is stored in a JSON file named badge_details.json. The JSON file contains the details of three badges: Data Novice, Data Explorer, and Analytics Enthusiast. Each badge has a set of students who have earned that badge.(This dataset is large around 10000+ badge holders)","Incorporate the new student named Sam into the Data Novice badge group.","Solution:","As we added only one student to set but the set is quite large(10000+ elements), we can save the set in a pickle file and assert it with the expected pickle file using assert_set_variable_equals_pickle() function.","This is how you can save the set in a pickle file.","Assertions:"]}],[{"l":"Pandas DataFrame Assertions by Examples","p":["In this notebook, you'll learn how to use most used Pandas Dataframe assertion functions. Below are these functions:","assert_pd_dataframe_variable_equals_variable(student_variable_name, expected_variable_name, delete_afterwards=True): Checks if student dataframe variable is equals to expected dataframe variable.","assert_pd_dataframe_variable_equals_csv(student_df_variable_name, colum_name, csv_name, read_csv_kwargs=None, series_testing_kwargs=None): Checks if student dataframe variable is equals to expected csv file.","assert_pd_dataframe_variable_column_equals_csv(student_df_variable_name, colum_name, csv_name, read_csv_kwargs=None, series_testing_kwargs=None): Checks if student dataframe variable column is equals to expected csv file.","assert_pd_dataframe_csv_equals_csv(student_csv_name, expected_csv_name, student_base_dir=., read_csv_kwargs=None, dataframe_testing_kwargs=None): Checks if student csv file is equals to expected csv file.","assert_pd_dataframe_variable_equals_pickle(student_variable_name, pickle_name, read_pickle_kwargs=None, dataframe_testing_kwargs=None): Checks if student dataframe variable is equals to expected pickle file. This is used when we have pivoted data or multi-column data in the dataframe.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["As the expected dataframe is small and we can easily compare it with the student dataframe, we can use assert_pd_dataframe_variable_equals_variable() function to assert the solution with the student dataframe.","Assertions:","Create a new column Price-to-Rating Ratio in the DataFrame that calculates the price-to-rating ratio for each book. This ratio will help us understand how the price of a book relates to its average rating.","Create a pivoted dataframe pivot_df from the df dataframe.","In this activity, we asked student to create a new column in the dataframe. So, we can use assert_pd_dataframe_variable_column_equals_csv() function to assert the solution with the student dataframe. We used this function to check the column price_to_rating with the expected column in the csv file.","In this activity, we asked student to create a pivoted dataframe from the original dataframe. So, we can use assert_pd_dataframe_variable_equals_pickle() function to assert the solution with the student dataframe because CSV file can't store the pivoted data.","In this activity, we asked student to remove the column from the dataframe. So, we can use assert_pd_dataframe_variable_equals_csv() function to assert the solution with the student dataframe.","In this activity, we asked student to save the dataframe in a new csv file. So, we can use assert_pd_dataframe_csv_equals_csv() function to assert the solution with the student dataframe.","Make sure not to reset the index","Now, with activities examples, you'll learn how to use the assertion functions. We'll use the df dataframe that contains the data of best books ever.","Save the updated dataframe df in a new CSV file named updated_best_book.csv. Save this file in current directory only.","Solution:","The \"isbn\" column is not needed for our analysis. Write a script to remove this column from the dataframe.","This is how you can save the dataframe to a new csv file.","This is how you can save the dataframe to a new pickle file."]}],[{"l":"Pandas Series Assertions by Examples","p":["In this you'll learn how to use most used Pandas Series assertion functions. Below are the functions that are covered.","assert_pd_series_variable_equals_variable(student_variable_name, expected_outcome_variable_name): Checks that the student's Series in student_variable_name matches the variable in expected_outcome_variable_name.","assert_pd_series_variable_equals_csv(student_variable_name, solution_csv_name, read_csv_kwargs=None): Checks that the student's Series in student_variable_name matches the Series contained in the solution CSV file named solution_csv_name.","assert_pd_series_variable_equals_pickle(student_variable_name, pickle_name, read_pickle_kwargs=None, series_testing_kwargs=None): Checks that the student's Series in student_variable_name matches the Series contained in the solution pickle file named pickle_name. This used when we have pivoted data or multi-column data.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions.","Use below data to create a series named student_data.","Solution:","As the expected output is small series, so we use assert_pd_series_variable_equals_variable() function to assert the solution with the student series.","Assertions:","Here, we passed first student variable then expected variable.","Create a series named prime_numbers_series which contains the first 10,000 prime numbers.","In previous example, the expected series is small, so we've used assert_pd_series_variable_equals_variable() function to assert the solution with the student series but in this example, the expected series is large, so we'll save the expected series to a csv file and then use assert_pd_series_variable_equals_csv() function to assert the solution with the csv file.","Create a series named student_data with below data and index.","Here, we'll use pickle file to save the expected series and then use assert_pd_series_variable_equals_pickle() function to assert the solution with the pickle file."]}],[{"l":"SQL Assertions by Examples","p":["In this notebook, you'll learn how to use most used SQL assertion functions. Below are the functions that I've covered in this notebook.","assert_sqlite_student_query_equals_expected_query(SQLITE_TRAVEL, SOLUTION_QUERY): Checks if student SQLite query is equals to expected query.","assert_mysql_student_query_equals_expected_query('sakila', SOLUTION_QUERY): Checks if student MySQL query is equals to expected query.","assert_postgresql_student_query_equals_expected_query('dvdrental', SOLUTION_QUERY): Checks if student PostgreSQL query is equals to expected query."]},{"l":"Activities"},{"l":"1. Find the First Country","p":["Write a query to retrieve the first country from the country table. Rename the column to First Country. Use IndepYear to determine the first country.","There are 3 SQL engine available, use the appropriate assertion function to check your query result with the expected result of the SOLUTION_QUERY.","For SQLite, use assert_sqlite_student_query_equals_expected_query(SQLITE_TRAVEL, SOLUTION_QUERY) function and the available database are:","SQLITE_CHINOOK(Chinook database)","SQLITE_NORTHWIND(Northwind database)","SQLITE_TRAVEL(Travel database)","SQLITE_ADVENTURE_WORKS(AdventureWorks database)","Here, all the databases are available: Spread Sheet Link"]}],[{"l":"Matplotlib Assertions by Examples","p":["In this notebook, you'll learn how to use most used matplotlib assertion functions. There is only one assertion function for checking the expected figure with the actual figure.","assert_plt_student_fig_matches_png_fname(student_figure_variable_name, expected_png_fname): Checks if student figure object is equals to expected png image.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions. We have df dataframe that contains the data of high school graduation rates and poverty rates across different U.S. states.","Create a visualization featuring two point plots on the same graph:","The first one representing normalized poverty rates, using df['normalized_poverty_rate'] and using a blue marker (that is already provided).","The second one representing normalized high schoool graduation rates, using df['normalized_hs_rate'], and using a red x marker (already provided).","This visualization will enable a comparative analysis of these two crucial socio-economic indicators across different states using Matplotlib.","Your visualization should have a size of (14, 7) and look something like:","Solution:","As we have to assert student variable with expected png image, we will use assert_plt_student_fig_matches_png_fname() function.","This is how you can save the figure and assert it with the expected image.","Assertions:","Create a scatter plot to visualize and analyze the relationship between the two key socio-economic indicators: the normalized high school graduation rate ( normalized_hs_rate) and the poverty rate( normalized_poverty_rate) across different U.S. states.","Use normalized_hs_rate in the X axis and normalized_poverty_rate in the Y axis. Your plot should look something like:"]}],[{"l":"PyTorch Assertions","p":["In this notebook, you'll learn how to use most used PyTorch assertion functions.","assert_pytorch_tensor_variable_name_equals_serialized(student_variable_name, location): Checks if student tensor object is equals to expected tensor. The expected tensor is stored in a pickle file.","assert_pytorch_tensor_variable_name_is_in_expected_device(student_variable_name, expected_device): Checks if student tensor object is in expected device. The expected device is a string.","assert_vision_transform_variable_name_equals_serialized(student_variable_name, location, **kwargs): Checks if student transformation pipeline object is equals to expected transformation pipeline. The expected transformation pipeline is stored in a pickle file.","assert_dataset_vision_image_folder_variable_name_equals_serialized(student_variable_name, location): Checks if student dataset(image directory) image folder object is equals to expected dataset image folder. The expected dataset image folder is stored in a pickle file.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions.","Use below list to create a 2d PyTorch tensor ans store it in a variable student.","Solution:","As we have created a tensor, save the tensor in a file using below code in a pickle file.","or, serialize the tensor using the below code.","Change the device of the tensor to cuda and store it in a variable student.","As we have changed the device of the tensor, save the tensor in a file using below code in a pickle file.","Assertions:","If student create a transformation pipeline with below parameters, then assert it with the expected transformation pipeline.","As we have created a transformation pipeline, save the transformation pipeline in a file using below code in a pickle file.","If student create a dataset image folder with below parameters, then assert it with the expected dataset image folder.","Here, the path/to/data is the path to the image folder and student is the transformation pipeline created in the previous activity example.","As we have created a dataset image folder, save the dataset image folder in a file using below code in a pickle file."]}],[{"l":"Numbers Assertions by Examples","p":["In this notebook, you'll learn how to use most used Numbers assertion functions. Below are these functions:","assert_variable_almost_equals(student_variable_name, expected_value, tolerance_in_decimals=2): Checks that the student's variable in student_variable_name matches the expected value within the given tolerance.","assert_variable_almost_equals_variable(student_variable_name, expected_variable_name, tolerance_in_decimals=2, delete_afterwards=True): Checks that the student's variable in student_variable_name matches the expected variable in expected_variable_name within the given tolerance.","Load the utils.py file to use the assertion functions."]},{"l":"Student Data for Activities","p":["In the activities, we'll use the below student data. Format of the data is name, marks, age, grade, subject.","There are total 500 students in the list, but we are showing only a few here."]},{"l":"Activities","p":["Loop through the list student_list, access the marks of the students who have English as their favorite subject, and calculate the average marks of those students. Store the average marks in the variable average_marks_english.","Solutions:","As the expected output is a float, so we use assert_variable_almost_equals() function to assert the solution with the student variable.","Assertion:","Loop through the list student_list, access the marks of the students who have Math as their favorite subject, and calculate the average marks of those students. Store the average marks in the variable average_marks_math.","As the expected output is a float, so we use assert_variable_almost_equals_variable() function to assert the solution with the student variable."]}],[{"l":"Working with Github","p":["Projects at DataWars are configured with a Github repository. Here's the overview of how it works:","Creating a github account","Creating a new repository","Project Structure","Reference: All artifacts in a Github repo","Automatic build/import actions","Transforming the project"]}],[{"l":"Transforming your project","p":["Remember to clear all the outputs of your Project.ipynb notebook before pushing."]}],[{"i":"automatic-buildimport-with-actions","l":"Automatic build/import with actions","p":["Once you push your project, the Github actions configured will be triggered automatically validating your project, building the docker image and importing the project into the DataWars platform."]}],[{"l":"Reference of Repo most important components","p":["Lab Content","english.md","chat.txt","public.md","docker-compose.yml"]}],[{"l":"Lab contents","p":["The left hand side of a project is the result of the english.md file. The right hand side, \"the lab\", is the result of the Docker image built during the project creation phase.","The process is advanced and you have a lot of control about how to build your image. But, for now, the only thing you have to know is that ANYTHING that you include in the notebooks/ directory will be included in the final lab image.","With the exception of Solution.ipynb and activity_solutions_files that have a special treatment."]},{"l":"Notebooks","p":["The main notebook you have to worry about for now is Project.ipynb, which is the \"stripped down/clean\" version of your `Solution.ipynb\"."]},{"l":"Adding your datasets","p":["Just include your datasets in the notebooks/ directory. I'd recommend just leaving them in the root of the folder. So, for example, upload the dataset to notebooks/pokemon.csv. The Project.ipynb notebook will then read it as pd.read_csv('pokemon.csv'), as Project.ipynb and pokemon.csv are in the same directory:","If you want, you can create a subdirectory, but this is not necessary (actually discouraged for simple datasets) for example:","In this case, the notebook will read the file as: pd.read_csv(data/pokemon.csv)."]},{"l":"Special libraries and requirements","p":["If your project uses any libraries outside of the regular stack from DataWars projects, you can include them in the requirements.txt"]},{"l":"Advanced usage","p":["This is only intended for advanced projects/users. You probably don't need it.","The Dockerfile contains the steps used to build the lab. You can fully customize this if required. The docker-compose.yml can contain special values:"]}],[{"i":"englishmd","l":"english.md","p":["The english.md file defines the content on the left panel of a project. It's a markdown file with special HTML tags defined by us. The main tags you have to know are page and activity. All tags must contain a UNIQUE ID (unique within the project).","You don't have to manually code the tags, as we have VSCode snippets that will make your life easier.","Download Snippets"]},{"l":"Pages","p":["Pages are MANDATORY and where all the other elements live. You can have as many pages as you want and they define the big \"sections\" on the left side panel. They can contain any markdown text or images you want to add as well as activities.","Pages are defined with the syntax:"]},{"l":"Activities","p":["Activities live within a page component. Depending on the activity type, they'll support special attributes and internal tags. But they all have a title attribute, and they support any text as a description:","As you can see from above, the activity description accepts any valid markdown code, including images.","All activities must contain the hint and solution tags. But you can leave them empty if you don't want to provide a hint/solution, as these are optional (although strongly recommended).","Specific activity types:"]},{"l":"Input activities","p":["Input activities ask the user for their answer by rendering a simple input widget. Here's the syntax:","When the user submits their answer, we check if the answer matches whatever you specified in correct-answer."]},{"l":"Multiple choice activities","p":["Here's the syntax of a multiple choice activity:","The only important thing to know is that the widget attribute will change the activity to accept only one correct answer (rendering HTML radio buttons) to multiple correct answers (rendering checkboxes). The accepted values are radio or checkbox respectively. Here's an example of a checkbox activity:"]},{"l":"Code Activities","p":["These are probably the most important and \"complicated\" activities. There are a lot details that can be configured for a code activity. But for now, we'll keep it simple and we'll document only a standard Jupyter activity.","You can ignore for now the attributes type, template and device as well as the metadata tag at the end. The only important tag for you to know is the validation-code one, which includes your assertions.","Never use the angle brackets and in the markdown content. They are used here to indicate the tags."]}],[{"i":"chattxt","l":"chat.txt","p":["The chat.txt file contains the context that will be passed to the AI assistant whenever the student asks a question. It should have the following structure:","The learning objectives are related to the skill your project covers. You can be more specific providing more details. For example, if you're working on Filtering and Sorting with Pandas, but your project heavily relies on boolean operators, you can be more specific and add:","The description of the data should be the characteristics of the data the student is dealing with. Be AS BRIEF as possible, as this counts towards the total TOKEN Count and is MORE EXPENSIVE. For example:","If your data is a tabular file (a CSV, dataframe), you can use the df.info() methods and just print the df.head().to_markdown().","If your data is a database, you can just tell the agent that \"the student is working with the MySQL Sakila sample database\".","Below is the example of a chat.txt file:","chat-txt","This example has 3 parts:","The objectives of the project.","The data the student has. (2-3 liner intro + first 10 rows of the data)","The info of the dataset.( df.info())"]}],[{"i":"publicmd","l":"public.md","p":["The public.md file contains the public description of a project in markdown format. Max length is 500 characters."]}],[{"i":"docker-composeyml","l":"docker-compose.yml","p":["This is a special file that won't be needed for now, only in exceptional cases. Leave it unchanged."]}],[{"l":"Github project structure","p":["This page contains a summary of each of the most important components in a project. The reference contains detailed information of each one of them."]},{"i":"1-englishmd","l":"1. english.md","p":["The MOST important file. It contains all the content that is rendered on the left side of the screen. It contains all the text of the pages and the activities.","Refer to the main documentation about english.md for a full description including the snippets to download for VS Code.","High level overview of the structure:","english.md","notebooks/","public.md","chat.txt","docker-compose.yml"]},{"i":"2-publicmd","l":"2. public.md","p":["This markdown file contains the public description of the project. Read more."]},{"i":"3-chattxt","l":"3. chat.txt","p":["The context for the AI Assistant, including objectives of the project and description of the data. Read more."]},{"l":"4. Notebooks and lab files","p":["The lab (right hand size) of your project will contains the notebooks, datasets, images, and other components needed. You can install custom requirements."]}],[{"l":"Getting paid","p":["To get paid, you need to send an invoice of the previous month to ap@datawars.io. The invoice has to be in US Dollars and contain your full name and instructions to get paid (your bank account details)."]},{"i":"example","l":"Example:","p":["The invoice should contain:"]}]]