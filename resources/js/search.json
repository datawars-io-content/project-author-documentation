[[{"l":"Welcome","p":["Welcome to the official DataWars' handbook for the Delivery team. It should be able to answer ALL your questions, and help you start on any position.","This is a work in progress. You are welcome and encouraged to collaborate and send Pull Requests to the main repo. If you find any problems, create an issue.","Here are a few quick links:","Are you a first time author starting our training? Go to first time authors docs."]}],[{"l":"Understanding DataWars","p":["DataWars Mission","The DataWars model","What makes a good project?","Structure of a project","What makes a good Dataset?","Examples of great projects"]}],[{"l":"DataWars Mission","p":["DataWars mission is to complement the abundant learning material that is out there (youtube videos, books, blogs, free courses, etc) with real life projects.","Our type of customer is someone that wants to learn by doing and build experience with Data Science. They work/study full time. This is going to be important by the time we discuss about what makes a good project."]}],[{"l":"DataWars Work Policy","p":["All employees must acknowledge they have read, understood, and agree to adhere to this policy.","All team members are expected to attend scheduled meetings, including daily scrums and other necessary team calls.","Any blockers or issues requiring assistance","Each author must post a daily update by the end of their workday. This update should include:","Each employee is accountable for their own work and must ensure that they meet deadlines and quality standards.","Employees must utilize the company-provided tools and resources for communication, project management, and collaboration. Ensure proper use of these tools to maintain productivity and efficiency.","Employees should respond to messages and emails from team members and stakeholders within a reasonable timeframe, preferably within 24 hours.","Failure to comply with this policy may result in performance reviews, and repeated non-compliance could lead to further disciplinary actions.","for daily updates (Slack)","If a team member is unable to attend a meeting, they must inform their team in advance and provide a brief update on their progress and plans on the same chanel for daily updates (Slack)","Regular check-ins with managers or team leads to discuss progress and address any issues are encouraged","Respect and support colleagues, fostering a positive and inclusive remote work environment.","Tasks completed","Tasks planned for the next day","This policy is designed to provide clear guidelines for remote work, ensuring that all team members, regardless of location, can work effectively and collaboratively. It aims to balance flexibility with accountability and maintain high standards of communication and responsiveness.","This policy will be reviewed regularly and updated as necessary to adapt to changing needs and circumstances.","Updates should be posted in the designated communication channel","While we do not enforce standard office hours, team members should overlap at least 4-5 hours with the core team working hours to facilitate meetings and collaboration.Considering everyone’s time zone and style of working - Let us be actively available for each other between 5:00 PM - 10:00 PM IST"]}],[{"l":"DataWars Leave Policy"},{"l":"Purpose","p":["This Paid Time Off (PTO) Policy outlines the guidelines for taking time off for various personal, health, and study reasons. The policy aims to provide employees with flexibility and ensure they maintain a healthy work-life balance."]},{"l":"Scope","p":["This policy applies to every full time, part time and contractual employee regardless of their position and department in the company"]},{"l":"Guidelines","p":["Employees are eligible for unlimited paid time off for health issues, including physical and mental health, and study purposes.","For health-related PTO, employees must notify their manager longer than 3 consecutive days. This will help to manage workload effectively. And, for unexpected illnesses, please keep your manager informed as soon as possible.","For study-related PTO, employees should submit their request at least 8 weeks in advance.","It is important to make sure that the team has enough resources to complete any pending tasks and keep the work going before implementing an unlimited PTO policy.","Misuse of the PTO policy will be addressed with appropriate disciplinary action, up to and including termination of employment","The Employee will have 12 Annual Leave, 5 Casual Leave in a year and can carry 5 numbers of leaves next year. The carried leaves can be encashed during their full and final settlement period.","Public Holidays are officially observed in India and Argentina. Any other country will be treated individually.","Employees shall submit the leave request here","Here are the Public Holiday List for 2024 - India& Argentina"]}],[{"l":"The datawars model","p":["A in DataWars is always enclosed in , which in turn is contained in a . Multiple are organized within a based on their and .","A denotes a single, atomic Skill that a user \"has\". For example, \"Dealing with null values\", or \"Dealing with duplicate values\". These are then logically aggregated in a . In this case, it'll be \"Data Cleaning\".","are subjective aggregations that we have decided to create.","A defines a \"role\" for a user, divided in multiple Learning Areas and Proficiencies. Within the juncture of Proficiency and Learning Area, you'll find a ."]}],[{"l":"What makes a good project?","p":["Understand the user's perspective an point of view","Starts slowly and progresses with the user, helping them navigate the complexity of the skill and learning about the dataset/challenge they're working with.","Starting slow will provide small dopamine injections that will keep the user engaged.","This will translate into a \"Flow\" state."]}],[{"l":"Structure of a project"}],[{"l":"What makes a good Dataset?","p":["A good dataset is 90% of the impact of a good project. A dataset that is interesting, engaging, of good quality is what keeps your students engaged.","Alignment with Project Goals: How well does the data address the specific research question or project objective?","Also, it’s very useful when we combine data from different sources to create a richer dataset. For example:","Complexity: have good relationships among variables/features.","Data Relevance:","Domain of work: for example, if you’re a biologist and know about some particular type of dataset that is publicly available for biology but nobody knows about it, it’ll make it more interesting.** Spark curiosity and potential","Fun, interesting, engaging: a dataset that teaches you something new aside from the topic you’re working on. For example, if you’re building a ML project, and using a DataSet about the impact of Nitrogen in Maize production, is something “unusual” and that most people will find interesting. Things about hobbies (music, movies, video games), engineering, science, biology will make things more interesting.","Go to the Australian Open Data portal and find the Electricity production of each power plant in the country. Then group per city to find total electricity per city. Then, find statistics about cities in the country (like population, income, weather, etc). Finally combine everything to make a dataset: Statistics of Australian cities 1990-2023.CSV","Go to the World Bank data site and find the production of rice per country. Then combine it with rainfall for all the countries. Finally, combine it with average temperatures. This would be AMAZING dataset: Rice production and weather data in the world.CSV","Good quality: Does it have enough rows and columns? Is there variety or all the columns are the same? This collaborates with the broader picture of the project.","Interesting stories (For exp, Cure the princes dataset or Livigno weather dataset, as it tells some stories behind the data)","Local/close to you: If you find datasets that are somehow closer to you, it’ll help you connect with your students. For example, I’m from Argentina and I found a dataset with all the Baby names used in Argentina.","Novelty: Data explores new ideas, events and areas","The key we want to focus on is the “qualitative properties” of the dataset:","The type of dataset (CSV, JSON, binary), the properties and features depends on the type of project you’re creating, so we won’t get into those details.","Topic Coverage: Does the data capture the key aspects and subtopics relevant to the chosen theme?"]},{"l":"Where can you get datasets from? Ideas of public websites","p":["Academic Publications and Research Papers: Reviewing relevant academic papers to identify potential data sources used by researchers in a particular field.","CDC (health US) Data Catalog: https://data.cdc.gov/browse","Data Science Blogs and Communities: Follow data science blogs, participate in online forums, and connect with other data scientists. This allows you to stay updated on new data repositories, emerging trends, and potential data sources shared by the community","data-is-plural.com: amazing weekly newsletter with access to a lot of datasets curated and explained","https://databank.illinois.edu/","https://dhsprogram.com/","https://ieee-dataport.org/- Interesting technical datasets. Requires a free account.","https://paperswithcode.com/- These are usually already replicated in Kaggle, but there are exceptions from time to time","https://ukdataservice.ac.uk/( https://internationaldata.ukdataservice.ac.uk/, https://stats2.digitalresources.jisc.ac.uk/)","https://www.pangaea.de/- Environmental datasets, a lot of time series and interesting climatic/chemical/biological datasets","https://www.societyforscience.org/research-at-home/large-data-sets/","https://zenodo.org/search- Mostly european and biological / climate, has good datasets.","Industry News and Publications: Stay informed about industry news and publications related to your domain. Announcements about new data initiatives or collaborations might reveal fresh data sources","Industry Reports and Datasets: Industry reports, whitepapers, or data published by companies or organizations related to the project theme. These might offer unique insights and datasets not readily available elsewhere.","It’s rather easy to find original and interesting datasets on the internet. We have done it for more than 100 projects already. We’re asking for your help because we’re growing and need the extra help. Here’s a list of places we usually get datasets from:","Keyword Research - Dataset based on list of keywords related to a particular subject/theme, such as (Agriculture - Crop dataset, soil health dataset or crop disease dataset, etc. For example, USDA NASS data portal)","Maven Analytic challenges: https://mavenanalytics.io/challenges","Niche data portals - things that are specific to your domain. For example, a cyber security data website, or the climatic data from a region in Italy.","Others channels where we can find data sources:","Recent Events and Activities: Utilize social media listening tools to identify relevant conversations and discussions happening online. Recent events like the country election[US Election], pandemic [Covid Dataset], etc. This can lead you to new data sources or user-generated content related to your topic","StackExchange& Reddit /r/datasets- Most of the time the datasets are already replicated to (or copied from) Kaggle. But there are some good gems from time to time.","Subject-Specific Repositories - these repositories are specific to our project's domain. For example, research data repositories might be available for healthcare, finance, or education. These can offer targeted datasets and resources relevant to our research question.","There’s also an API and a repo","UC Berkeley data: https://dlab.berkeley.edu/data/uc-data","US National Park Service data: huge catalog of data associated with US National Parks","World Bank Data- most datasets are uninteresting, but there are some gems as well","Your country’s data portal - this is ideal to find things that are unique to you. For example, the UK data portal, Ireland’s data portal, or the Argentina data portal. There’s a prevalent list here."]},{"l":"Dataset Quality Framework","p":["Metrics for evaluating dataset quality. These metrics can be implemented through a scoring system or a checklist. A point system for each metric, with higher scores indicating better quality. Alternatively, a checklist can ensure all essential criteria are met for a \"good\" dataset.","Quality assurance and progress monitoring is implemented using spreadsheets.","Volume and variety: Dataset should contains enough rows (~1000), features and data points","Accuracy and cleanliness: The dataset should be reliable and free from errors(no inconsistency and unnecessary columns or rows)","Completeness: Dataset with minimal missing values and outliers is preferred ( more missing values introduce biases and hinder analysis)","Source Credibility: Evaluation of the data source's reputation and reliability. ( government portal generally have high credibility than public data portals and sources)","Documentation Clarity: How well does the accompanying documentation explain the data structure, variables, and usage?"]}],[{"l":"Understanding the scope of a project","p":["It's VERY important to understand the scope of the project you're working with (given by the Skill)."]}],[{"l":"Examples of great projects"}],[{"i":"#","p":["A comprehensive guide for maintaining good communication, meeting etiquette, and preparedness within the workspace."]},{"l":"Good Workspace Environment"},{"l":"Communication Best Practices"},{"l":"Primary Tools:","p":["We primarily use Google Workspace(Gmail, Google Docs, Google Sheets, etc.) for all official documentation, communication, and collaboration.","Slack is our day-to-day communication tool, but it is not a formal platform for critical or permanent information. Slack is informal; if anything is important, it must be documented in an email or stored in Google Workspace.","Discord is completely unofficial. If you send anything in Discord, assume it will not be read by anyone."]},{"l":"Managing Important Information:","p":["All crucial information—such as decisions, updates, or records—must live in a formal system (Google Docs, Google Drive, Gmail) to ensure transparency, traceability, and easy access.","For actionable tasks or project updates, we use Airtable."]},{"l":"Google Calendar Etiquette"},{"l":"Using Google Calendar for Meetings:","p":["Mandatory Meetings: All scheduled meetings (weekly syncs, check-ins, team discussions) are mandatory.","Missed Meetings: Missing a meeting without prior notice is unacceptable. If unable to attend, you must notify the organizer in advance with a valid reason."]},{"l":"Meeting Preparedness & Responsibility","p":["Webcam Policy: All employees are expected to keep their webcams ON during meetings to foster engagement. Turning off your camera should be an exception, not the norm.","Audio Quality: Clear and reliable audio is essential. Employees should verify their audio setup before meetings to ensure effective communication.","Environment Setup for Meetings:","Ensure you are in a quiet, well-lit environment during meetings to minimize distractions.","Be mindful of background noise and visual clutter, as they can affect focus and professionalism during meetings.","If you experience issues with your setup, DataWars can provide the necessary equipment (webcams, microphones, etc.). However, it is your responsibility to request assistance if needed."]},{"l":"Who Should You Contact","p":["Your first point of contact is Anurag Verma averma@datawars.io. You can escalate issues to Santiago, but only after Anurag has given approval or if you are not getting a response."]}],[{"l":"Assertion Framework Documentation","p":["In this assertion framework documentation, you'll learn how to use already defined function to check/assert the activity.","This assertion framework documentation is divided into two parts, first part cover the most used assertion function with example activities which teach you how to save the expected result and then check it with users/student solution. Second part cover the not only how to use them but in more details like what are the parameters, what are the return types, etc.","Make to use the same versions of required libraries as used in the platform. You can check the version of the library by running the following command in the notebook cell:","Similarly, you can check the version of other libraries like numpy, matplotlib, etc.","Functions Assertion","Python List Assertion","Python Tuple Assertion","Python Dictionary Assertion","Python Sets Assertion","Pandas Series Assertion","Pandas DataFrame Assertion","SQL Assertion","Matplotlib Assertion","PyTorch Assertion","Numbers Assertion"]}],[{"l":"Numbers Assertions","p":["In this notebook, you'll learn how to use most used Numbers assertion functions. Below are these functions:","assert_variable_almost_equals(student_variable_name, expected_value, tolerance_in_decimals=2): Checks that the student's variable in student_variable_name matches the expected value within the given tolerance.","assert_variable_almost_equals_variable(student_variable_name, expected_variable_name, tolerance_in_decimals=2, delete_afterwards=True): Checks that the student's variable in student_variable_name matches the expected variable in expected_variable_name within the given tolerance.","Load the utils.py file to use the assertion functions."]},{"l":"Student Data for Activities","p":["In the activities, we'll use the below student data. Format of the data is name, marks, age, grade, subject.","There are total 500 students in the list, but we are showing only a few here."]},{"l":"Activities","p":["Loop through the list student_list, access the marks of the students who have English as their favorite subject, and calculate the average marks of those students. Store the average marks in the variable average_marks_english.","Solutions:","As the expected output is a float, so we use assert_variable_almost_equals() function to assert the solution with the student variable.","Assertion:","Loop through the list student_list, access the marks of the students who have Math as their favorite subject, and calculate the average marks of those students. Store the average marks in the variable average_marks_math.","As the expected output is a float, so we use assert_variable_almost_equals_variable() function to assert the solution with the student variable."]}],[{"l":"Function Assertions","p":["In this notebook, you'll learn how to use most used Functions checking assertion functions. Below are the functions that I've covered in this notebook.","assert_student_function_name_equals(student_function_name, fn_args=None, fn_kwargs=None, expected_value=DataWarsConstants.EMPTY): Checks that the student's function named student_function_name returns the expected value.","assert_student_function_test_cases(student_function_name, test_cases): Checks that the student's function named student_function_name returns the expected value for each test case in the list test_cases.","assert_student_function_test_cases_dictionary(student_function_name, test_cases): Similar to assert_student_function_test_cases, but each test case is a dictionary in the list test_cases. This allows you to specify positional and keyword arguments, expected return values, or expected exceptions (with optional messages).","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions.","Create a functions add() that takes two arguments and returns the sum of the two arguments.","Solution:","Out expected function should return the sum of the two arguments. So, we use assert_student_function_name_equals() function to assert the solution with the student function.","Assertions:","Define a function define_movies that takes a dictionary, id, name, year, and rank as arguments and returns the dictionary with the movie details. Function also takes empty dictionary as first argument which will be used to store the movie details.","Here we have two expected outputs, so we save them in two different dictionaries and then use assert_student_function_test_cases() function to assert the solution with the student function. Assertions:","Define a function define_roles that takes a dictionary, actor_id, and role as arguments and returns the dictionary with the actor's roles. Function also takes empty dictionary as first argument which will be used to store the actor's roles.","Here we have two expected outputs, so we save them in two different dictionaries and then use assert_student_function_test_cases() function to assert the solution with the student function.","Create a function divide_numbers(a, b) that returns the result of a / b. Raise a ZeroDivisionError if b == 0.","Here we use assert_student_function_test_cases_dictionary() to check return values and exception handling."]}],[{"l":"List Assertions","p":["In this notebook, you'll learn how to use most used List assertion functions. Below are these functions:","assert_list_variable_equals_variable(student_variable_name, expected_variable_name, delete_afterwards=True): Checks that the student's list in student_variable_name matches the list in expected_variable_name.","assert_list_variable_equals_json(student_variable_name, json_file_name): Checks that the student's list in student_variable_name matches the list contained in the solution JSON file named json_file_name.","assert_list_variable_equals_pickle(student_variable_name, pickle_file_name): Checks that the student's list in student_variable_name matches the list contained in the solution pickle file named pickle_file_name.","Load the utils.py file to use the assertion functions."]},{"l":"Student Data for Activities","p":["In the activities, we'll use the below student data. Format of the data is name, marks, age, grade, subject.","There are total 500 students in the list, but we are showing only a few here."]},{"l":"Activities","p":["As the expected output is a list of integers containing total of 500 students and it is a large list, so we use assert_list_variable_equals_json() function to assert the solution with the student list.","As the expected output is a list of tuples which is small, so we use assert_list_variable_equals_pickle() function to assert the solution with the student list as JSON can't save tuples.","As we can see the expected output is a list of list which is small(only 6 students), so we use assert_list_variable_equals_variable() function to assert the solution with the student list.","Assertion:","Assertions:","Convert the ages into int as currently they are type str(string).","Create a list select_student from the original list( student_list), starting at index 98 up to, but not including, index 104.","Create two lists a_graders and b_graders with the name, age, grade, and favorite subject of the students. Put students with A grades to the list a_graders and students with B graders to the list b_graders","Expected output for a_graders:","Expected output:","Given the list student_list is a list of lists. Convert it into a list of tuples and store it in the variable student_list_tuple.","Here, we have two expected outputs, so we save them in two different json files and then use assert_list_variable_equals_json() function to assert the solution with the student list.","Loop through the list student_list, access the age, cast it to int, and append it to the new list variable students_age.","Make sure to not change the order of the students.","Solution:","Solutions:","This is how we can save the expected output in a json file:","This is how we can save the expected output in a pickle file:","This is just a simple conversion from list of lists to list of tuples. Expected output is different from above example list of tuples.","We use assert_list_variable_equals_json() function to assert the solution with the student list as the expected output is a list of list which is is quite large."]}],[{"l":"Tuple Assertions","p":["In this notebook, you'll learn how to use most used Tuple assertion functions. Below are these functions:","assert_tuple_variable_equals_variable(student_variable_name, expected_variable_name, delete_afterwards=True): Checks if student tuple variable is equals to expected tuple variable.","assert_tuple_variable_equals_pickle(student_variable_name, pickle_file_name): Checks if student tuple variable is equals to expected pickle file.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Below are two tuples having students records of two cohorts for Computer Science course this semester, each cohort has 30 students","Each tuple is as (student_name, overall_grade).","Combine the records of both cohorts into one tuple named combined.","Solution:","As the expected tuple is quite large, we can save it in a pickle file and then use assert_tuple_variable_equals_pickle() function to assert the solution with the student tuple.","This is how you can save the tuple in a pickle file.","Assertions:","First, find the highest grade and then find the student with that grade then store the details in a tuple named top_student.","As the expected tuple only contains one student, we can easily compare it with the student tuple, we can use assert_tuple_variable_equals_variable() function to assert the solution with the student tuple."]}],[{"l":"Dict Assertions","p":["In this you'll learn how to use most used Dictionary assertion functions. Below are the functions that are covered.","assert_dict_variable_equals_variable(student_variable_name, expected_variable_name, delete_afterwards=True): Checks that the student's dictionary in student_variable_name matches the dictionary in expected_variable_name.","assert_dict_variable_equals_json(student_variable_name, json_file_name): Checks that the student's dictionary in student_variable_name matches the dictionary contained in the solution JSON file named json_file_name.","assert_dict_variable_equals_pickle(student_variable_name, pickle_file_name): Checks that the student's dictionary in student_variable_name matches the dictionary contained in the solution pickle file named pickle_file_name.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions.","We'll use the below fruits data for the activities.","Use below data to create a dictionary named student_dict.","Expected Output:","Solution:","As the expected output is small dictionary, so we use assert_dict_variable_equals_variable() function to assert the solution with the student dictionary.","Assertions:","Use the fruits_data list to create a dictionary named fruits where the key is the fruit name and the value is a dictionary containing the color and price of the fruit.","In the previous activity, we used assert_dict_variable_equals_variable() function to assert the solution with the student dictionary. But here the expected output is large, so we use assert_dict_variable_equals_json() function to assert the solution with the student dictionary.","Store the result in a variable red_fruits.","Store the result in a variable red_blue_fruits, here the key is the color and value is the tuple containing two values - color and its price.","As we know that when we save tuple in json file, it will be saved as list. So, we use pickle file to save the expected output and then use assert_dict_variable_equals_pickle() function to assert the solution with the student dictionary."]}],[{"l":"Sets Assertions","p":["In this notebook, you'll learn how to use most used Sets assertion functions. Below are these functions:","assert_set_variable_equals_variable(student_variable_name, expected_variable_name, delete_afterwards=True): Checks if student set variable is equals to expected set variable.","assert_set_variable_equals_json(student_variable_name, json_file_name): Checks if student set variable is equals to expected json file.","assert_set_variable_equals_pickle(student_variable_name, pickle_file_name): Checks if student set variable is equals to expected pickle file.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions.","You have two sets of students enrolled in two different courses. The sets are as follows:","Find the common students enrolled in both courses.","Solution:","As the expected set is small and have only few elements, we can easily compare it with the student set, we can use assert_set_variable_equals_variable() function to assert the solution with the student set. Assertions:"]},{"l":"Activities on Badge Data","p":["For other assertions examples, we will use the badge data. The badge data is stored in a JSON file named badge_details.json. The JSON file contains the details of three badges: Data Novice, Data Explorer, and Analytics Enthusiast. Each badge has a set of students who have earned that badge.(This dataset is large around 10000+ badge holders)","Incorporate the new student named Sam into the Data Novice badge group.","Solution:","As we added only one student to set but the set is quite large(10000+ elements), we can save the set in a pickle file and assert it with the expected pickle file using assert_set_variable_equals_pickle() function.","This is how you can save the set in a pickle file.","Assertions:"]}],[{"l":"Pandas Series Assertions","p":["In this you'll learn how to use most used Pandas Series assertion functions. Below are the functions that are covered.","assert_pd_series_variable_equals_variable(student_variable_name, expected_outcome_variable_name): Checks that the student's Series in student_variable_name matches the variable in expected_outcome_variable_name.","assert_pd_series_variable_equals_csv(student_variable_name, solution_csv_name, read_csv_kwargs=None): Checks that the student's Series in student_variable_name matches the Series contained in the solution CSV file named solution_csv_name.","assert_pd_series_variable_equals_pickle(student_variable_name, pickle_name, read_pickle_kwargs=None, series_testing_kwargs=None): Checks that the student's Series in student_variable_name matches the Series contained in the solution pickle file named pickle_name. This used when we have pivoted data or multi-column data.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions.","Use below data to create a series named student_data.","Solution:","As the expected output is small series, so we use assert_pd_series_variable_equals_variable() function to assert the solution with the student series.","Assertions:","Here, we passed first student variable then expected variable.","Create a series named prime_numbers_series which contains the first 10,000 prime numbers.","In previous example, the expected series is small, so we've used assert_pd_series_variable_equals_variable() function to assert the solution with the student series but in this example, the expected series is large, so we'll save the expected series to a csv file and then use assert_pd_series_variable_equals_csv() function to assert the solution with the csv file.","Create a series named student_data with below data and index.","Here, we'll use pickle file to save the expected series and then use assert_pd_series_variable_equals_pickle() function to assert the solution with the pickle file."]}],[{"l":"Pandas DataFrame Assertions","p":["In this notebook, you'll learn how to use most used Pandas Dataframe assertion functions. Below are these functions:","assert_pd_dataframe_variable_equals_variable(student_variable_name, expected_variable_name, delete_afterwards=True): Checks if student dataframe variable is equals to expected dataframe variable.","assert_pd_dataframe_variable_equals_csv(student_df_variable_name, colum_name, csv_name, read_csv_kwargs=None, series_testing_kwargs=None): Checks if student dataframe variable is equals to expected csv file.","assert_pd_dataframe_variable_column_equals_csv(student_df_variable_name, colum_name, csv_name, read_csv_kwargs=None, series_testing_kwargs=None): Checks if student dataframe variable column is equals to expected csv file.","assert_pd_dataframe_csv_equals_csv(student_csv_name, expected_csv_name, student_base_dir=., read_csv_kwargs=None, dataframe_testing_kwargs=None): Checks if student csv file is equals to expected csv file.","assert_pd_dataframe_variable_equals_pickle(student_variable_name, pickle_name, read_pickle_kwargs=None, dataframe_testing_kwargs=None): Checks if student dataframe variable is equals to expected pickle file. This is used when we have pivoted data or multi-column data in the dataframe.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["As the expected dataframe is small and we can easily compare it with the student dataframe, we can use assert_pd_dataframe_variable_equals_variable() function to assert the solution with the student dataframe.","Assertions:","Create a new column Price-to-Rating Ratio in the DataFrame that calculates the price-to-rating ratio for each book. This ratio will help us understand how the price of a book relates to its average rating.","Create a pivoted dataframe pivot_df from the df dataframe.","In this activity, we asked student to create a new column in the dataframe. So, we can use assert_pd_dataframe_variable_column_equals_csv() function to assert the solution with the student dataframe. We used this function to check the column price_to_rating with the expected column in the csv file.","In this activity, we asked student to create a pivoted dataframe from the original dataframe. So, we can use assert_pd_dataframe_variable_equals_pickle() function to assert the solution with the student dataframe because CSV file can't store the pivoted data.","In this activity, we asked student to remove the column from the dataframe. So, we can use assert_pd_dataframe_variable_equals_csv() function to assert the solution with the student dataframe.","In this activity, we asked student to save the dataframe in a new csv file. So, we can use assert_pd_dataframe_csv_equals_csv() function to assert the solution with the student dataframe.","Make sure not to reset the index","Now, with activities examples, you'll learn how to use the assertion functions. We'll use the df dataframe that contains the data of best books ever.","Save the updated dataframe df in a new CSV file named updated_best_book.csv. Save this file in current directory only.","Solution:","The \"isbn\" column is not needed for our analysis. Write a script to remove this column from the dataframe.","This is how you can save the dataframe to a new csv file.","This is how you can save the dataframe to a new pickle file."]}],[{"l":"Matplotlib Assertions","p":["In this notebook, you'll learn how to use most used matplotlib assertion functions. There is only one assertion function for checking the expected figure with the actual figure.","assert_plt_student_fig_matches_png_fname(student_figure_variable_name, expected_png_fname): Checks if student figure object is equals to expected png image.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions. We have df dataframe that contains the data of high school graduation rates and poverty rates across different U.S. states.","Create a visualization featuring two point plots on the same graph:","The first one representing normalized poverty rates, using df['normalized_poverty_rate'] and using a blue marker (that is already provided).","The second one representing normalized high schoool graduation rates, using df['normalized_hs_rate'], and using a red x marker (already provided).","This visualization will enable a comparative analysis of these two crucial socio-economic indicators across different states using Matplotlib.","Your visualization should have a size of (14, 7) and look something like:","Solution:","As we have to assert student variable with expected png image, we will use assert_plt_student_fig_matches_png_fname() function.","This is how you can save the figure and assert it with the expected image.","Assertions:","Create a scatter plot to visualize and analyze the relationship between the two key socio-economic indicators: the normalized high school graduation rate ( normalized_hs_rate) and the poverty rate( normalized_poverty_rate) across different U.S. states.","Use normalized_hs_rate in the X axis and normalized_poverty_rate in the Y axis. Your plot should look something like:"]}],[{"l":"NumPy Array Assertion Functions","p":["In this notebook, you’ll learn how to use the most common NumPy array assertion functions. Below are these functions:","assert_variable_type_np_array(var_name) Checks whether a variable in globals() is a NumPy array (i.e., np.ndarray).","assert_np_array_equal(student_array, expected_array, exact=True, rtol=1e-7, atol=0) Compares two arrays either exactly or approximately(if exact=False).","Exact checks each element is identical.","Approximate uses tolerances ( rtol, atol) for floating-point comparisons.","assert_np_array_variable_equals_variable(student_var_name, expected_var_name, **kwargs) Verifies two global variables contain matching arrays. Optionally set exact=False for approximate checks.","assert_np_array_variable_equals(student_var_name, expected_array, **kwargs) Compares a global variable(student’s result) to an in-memory array(teacher’s or reference result).","assert_np_array_equals_pickle(student_array, pickle_name, read_pickle_kwargs=None, array_testing_kwargs=None) Loads a pickle file containing a NumPy array, then compares it to student_array.","assert_np_array_variable_equals_pickle(student_variable_name, pickle_name, read_pickle_kwargs=None, array_testing_kwargs=None) Loads a pickle file containing a NumPy array, then compares it to a global variable named student_variable_name.","assert_np_array_variable_equals_parquet(student_variable_name, parquet_name, read_parquet_kwargs=None, array_testing_kwargs=None) Loads a Parquet file containing a NumPy array, then compares it to a global variable named student_variable_name.","assert_np_array_variable_equals_npy(student_variable_name, npy_name, array_testing_kwargs=None) Loads a .npy file containing a NumPy array, then compares it to a global variable named student_variable_name."]},{"l":"Loading the Utility","p":["To use these assertion functions, ensure they are defined or imported in your notebook:","Now you can start asserting student solutions with your reference arrays or data files."]},{"l":"Activities","p":["After the assertion, expected_result is deleted from globals() to avoid polluting the environment.","Because we didn’t pass exact=False, the check is performed element-by-element with no tolerance for floating-point differences.","Below are sample activities demonstrating typical usage of these NumPy assertion functions. Each activity shows an example solution and the assertion that validates it.","If my_array isn’t found in globals() or is not an np.ndarray, it raises an AssertionError.","If the differences are within rtol=1e-3, the test passes.","If the file is missing, or if the unpickled data is not a NumPy array, or if the arrays differ in shape or values, it raises AssertionError.","Task: Create a global variable my_array that should be a NumPy array. If it’s not (e.g., a list, dict, etc.), it should fail.","Task: Create a variable floats_approx close to [1.0, 2.0, 3.0] but with small floating differences.","Task: Create an array named student_arr that exactly matches [1, 2, 3].","Task: Load a NumPy array from a file named data.npy and store it in a variable student_data.","Task: Load a Parquet file named data.parquet that contains a NumPy array, and store it in a variable student_data.","Task: Load a pickle file named array_data.pkl that contains a 2D array, and store it in a variable student_matrix.","Task: Suppose the student’s code populates my_pickled_data in globals() and you want to confirm it matches the official reference data stored in official_data.pkl.","Task: Suppose you produce two arrays, student_result and expected_result, in globals(). Ensure they match exactly.","This assertion checks if the student’s array matches the one stored in the .npy file.","This assertion checks if the student’s array matches the one stored in the Parquet file.","This ensures the student’s loaded array matches the original in the pickle."]},{"l":"Additional Notes","p":["Pass exact=False, rtol=..., atol=... to any of these functions if you need approximate floating-point comparisons.","If your pickle contains something other than a NumPy array (e.g., a Python list), AssertionError is raised.","If you want to compare arrays with complex data types or object arrays, these functions still work; any mismatch in shape or values triggers a fail."]}],[{"l":"SQL Assertions","p":["In this notebook, you'll learn how to use most used SQL assertion functions. Below are the functions that I've covered in this notebook.","assert_sqlite_student_query_equals_expected_query(SQLITE_TRAVEL, SOLUTION_QUERY): Checks if student SQLite query is equals to expected query.","assert_mysql_student_query_equals_expected_query('sakila', SOLUTION_QUERY): Checks if student MySQL query is equals to expected query.","assert_postgresql_student_query_equals_expected_query('dvdrental', SOLUTION_QUERY): Checks if student PostgreSQL query is equals to expected query."]},{"l":"Activities"},{"l":"1. Find the First Country","p":["Write a query to retrieve the first country from the country table. Rename the column to First Country. Use IndepYear to determine the first country.","There are 3 SQL engine available, use the appropriate assertion function to check your query result with the expected result of the SOLUTION_QUERY.","For SQLite, use assert_sqlite_student_query_equals_expected_query(SQLITE_TRAVEL, SOLUTION_QUERY) function and the available database are:","SQLITE_CHINOOK(Chinook database)","SQLITE_NORTHWIND(Northwind database)","SQLITE_TRAVEL(Travel database)","SQLITE_ADVENTURE_WORKS(AdventureWorks database)","Here, all the databases are available: Spread Sheet Link"]}],[{"l":"PyTorch Assertions","p":["In this notebook, you'll learn how to use most used PyTorch assertion functions.","assert_pytorch_tensor_variable_name_equals_serialized(student_variable_name, location): Checks if student tensor object is equals to expected tensor. The expected tensor is stored in a pickle file.","assert_pytorch_tensor_variable_name_is_in_expected_device(student_variable_name, expected_device): Checks if student tensor object is in expected device. The expected device is a string.","assert_vision_transform_variable_name_equals_serialized(student_variable_name, location, **kwargs): Checks if student transformation pipeline object is equals to expected transformation pipeline. The expected transformation pipeline is stored in a pickle file.","assert_dataset_vision_image_folder_variable_name_equals_serialized(student_variable_name, location): Checks if student dataset(image directory) image folder object is equals to expected dataset image folder. The expected dataset image folder is stored in a pickle file.","Load the utils.py file to use the assertion functions."]},{"l":"Activities","p":["Now, with activities examples, you'll learn how to use the assertion functions.","Use below list to create a 2d PyTorch tensor ans store it in a variable student.","Solution:","As we have created a tensor, save the tensor in a file using below code in a pickle file.","or, serialize the tensor using the below code.","Change the device of the tensor to cuda and store it in a variable student.","As we have changed the device of the tensor, save the tensor in a file using below code in a pickle file.","Assertions:","If student create a transformation pipeline with below parameters, then assert it with the expected transformation pipeline.","As we have created a transformation pipeline, save the transformation pipeline in a file using below code in a pickle file.","If student create a dataset image folder with below parameters, then assert it with the expected dataset image folder.","Here, the path/to/data is the path to the image folder and student is the transformation pipeline created in the previous activity example.","As we have created a dataset image folder, save the dataset image folder in a file using below code in a pickle file."]}],[{"i":"#","p":["Step-by-step guide to modifying the box size for a custom module via the Django Admin interface."]},{"l":"Update Box Size in Datawars Teams Platform"},{"l":"How to Update Box Size in the Datawars Teams Platform ?","p":["To update the box size for a custom module in the Datawars Teams platform, follow these steps:","Identify the Custom Module ID Locate the ID of the custom module you want to update. Custom Module ID","Access the Django Admin Panel Open the following URL in your browser, replacing {custom_module_id} with the actual module ID:","Update the Box Size","Scroll down to find the Stack size field.","Update the value of this field, and save the changes.","\uD83D\uDCA1 Tip: Set the box size to the smallest possible value that meets your needs. Select Stack Size"]}],[{"l":"Content creation process","p":["Must link to Good Practices for authors to strictly follow the guidelines."]}],[{"l":"Project Testing Guide for New Testers","p":["First go through Good Practices for Project Authoring."]},{"l":"The \"Detective's Mindset\" for Testing","p":["Imagine you are a detective, stepping into a mysterious case. The project before you is like a crime scene - every detail matters. Typos are the dropped breadcrumbs, unclear instructions are the smudged fingerprints, and incorrect solutions? They’re the smoking gun. With your magnifying glass in hand and an insatiable curiosity, your job is to uncover every imperfection, every inconsistency, and every missed opportunity.","You revel in the thrill of discovery. There’s nothing quite like the satisfaction of finding a flaw no one else noticed. Below are the ways in which you can do so:"]},{"l":"Areas to Target Relentlessly"},{"l":"1. Spelling and Grammar Mistakes","p":["What to Look For: Spelling and grammar errors are easy catches, and even one mistake can leave a bad impression.","Why it Matters: Poor language skills make the project look unpolished and can frustrate users.","Your Task: Review every sentence carefully. Hunt down each misspelling, typo, or awkward phrase. Don't let them slip up on the basics."]},{"l":"2. Relevance and Accuracy of Introductory Content","p":["Objective: Make sure that the Introduction section and public.md are on-topic and give a clear overview of the project. public.md is the first thing a user sees before starting the project.","Checklist:","Is the introduction relevant to the actual project content? Look for any vague language or misleading details.","Ensure that all information is correct and aligned with the project's purpose. Set the standard for a strong first impression.","Introduction and public.md are visible at following places in the project. Introduction public.md"]},{"l":"3. Activity Descriptions","p":["Purpose: The activity description should leave no room for confusion or misinterpretation.","What to Watch For:","Clarity: If you see dense, jargon-filled paragraphs, insist they simplify. No user should feel lost while reading.","Conciseness: A description should be as short as possible while still clear. If long explanations are necessary, they should be broken into bullet points.","Completeness: Are the instructions thorough? If any step or detail is vague or missing, report it. Authors should know that clarity is non-negotiable."]},{"l":"4. Expected Outcome","p":["Importance: Users rely on a clear expected outcome to see if they're on the right track.","What to Verify:","Check that the expected outcome exactly matches the solution. There's no room for \"almost.\"","If results might vary due to a solution that results in dynamic result, ensure there's a note explaining this. Users should know what to expect or how to judge their answers if they differ from the example.","The example below demonstrates the correct expected outcome. Correct Expected Outcome"]},{"l":"5. Solution Quality and Explanation","p":["Goal: A solution should do more than just provide an answer - it should make the answer understandable.","Essentials:","Code Comments: Ensure the solution code is well-commented so each step is clear. If anything is unclear, insist on explanations.","MCQ Activities: For multiple-choice questions, a simple correct answer isn't enough. Users need to understand why an answer is correct. If the explanation is missing, it's an issue.","The screenshot below illustrates an incorrect way of presenting a solution to the user. While it provides the final answer, it fails to show the necessary steps to arrive at that conclusion, leaving the user without proper guidance. Bad Solution"]},{"l":"6. Jupyter Notebook Content","p":["Objective: The Jupyter notebook should be carefully crafted to guide users without giving away answers.","What to Look For:","Ensure there are no hints or solutions provided within the notebook that the authors didn't intend to reveal.","Check that there are appropriate placeholders or empty code cells for users to fill in their own answers. This helps users engage actively.","In the following example, the author has left a hint in the Jupyter notebook. This is an issue. Hint or solution should not be present in the jupyter notebook.","The example below highlights the absence of placeholders in the Jupyter notebook, which can lead to confusion. Absence of proper placeholders","After adding placeholders in the previous example, we get clear, complete and easy to follow activities. Correct Placeholders"]},{"l":"Is It Enough?","p":["After meticulously investigating and uncovering every flaw in the project, you might feel a surge of accomplishment. But after a while, the thrill of finding errors alone starts to fade. The case doesn’t feel closed. Something is missing.","You’ve exposed every clue, but you haven’t solved the whole mystery. Instead of stopping at fault-finding, consider what a truly great detective would do: collaborate with the \"suspects\" to rewrite the story. Help the project authors understand not just what went wrong but why it went wrong and how to fix it. Debug their code, offer insights, and provide actionable feedback that saves them time and effort.","When you take this approach, you’re no longer just a critic - you’re a partner."]},{"l":"How to Report Issues?","p":["The following video serves as guide on how to report issues on the platform."]}],[{"l":"Datawars Dataset","p":["This page provides a step-by-step guide to creating datasets for Datawars playgrounds."]},{"l":"Step 1: Finding datasets"},{"l":"Dataset can be found on various data sources. Here are the ways for finding datasources. Searce for repository with relevant keyword. There are public and government data repository that provide data of different domain.","p":["For exp: GBIF Global Biodiversity Information Facility—is an international network and data infrastructure aimed at providing anyone, anywhere, open access to data about biodiversity and all types of life on Earth.","Academic Publications and Research Papers: Reviewing relevant academic papers to identify potential data sources used by researchers in a particular field.","Industry Reports and Datasets: Industry reports, whitepapers, or data published by companies or organizations related to the project theme. These might offer unique insights and datasets not readily available elsewhere.","Recent Events and Activities: Utilize social media listening tools to identify relevant conversations and discussions happening online. Recent events like the country election[US Election], pandemic [Covid Dataset], etc. This can lead you to new data sources or user-generated content related to your topic.","Data Science Blogs and Communities: Follow data science blogs, participate in online forums, and connect with other data scientists. This allows you to stay updated on new data repositories, emerging trends, and potential data sources shared by the community","Industry News and Publications: Stay informed about industry news and publications related to your domain. Announcements about new data initiatives or collaborations might reveal fresh data sources"]},{"l":"Step 2: Extracting and processing datasets","p":["Dataset can be downloaded from the source, in suitable format avilable like CSV, JSON, XML, etc. In next step, data files can be transformed into structured format such as CSV."]},{"l":"Step 3: Writing dataset description","p":["Add images to make the description more interactive","Application of dataset: Mention the possible application domains of the dataset. This helps in understanding the dataset better. Applications of dataset","Data path: Mention the path of the dataset. This is important as it helps in locating the dataset easily.","Data Source Path","Database path is: {{devices.\"Data Source\".ip_address}}","Dataset path is: {{datasource.filesystem_path}}/\"filename\"","Dataset use example: Mention the possible use case of the dataset. Use cases","df.head() Adding preview of the dataset gives a brief idea about the dataset. Dataset Preview","df.info(): This method prints information about a DataFrame including the index dtype and column dtypes, non-null values, and memory usage. Info of Dataset","For data files with CSV file format, the path can be mentioned as follows: {{datasource.filesystem_path}}/\"filename\"","for data files with csv format: WHO Malaria Report 2023","For databases, the ip address path can be written as follows: {{devices_copy.\"Data Source\".ip_address}} Dataset path","for databases: Employees Database","For Example:","For Refrence to dataset description, here are some of the description file:","Here is the explample file for the dataset description: Querying Logical Operator with Airlines using Python","Here is the path to get ip address of devices rendered in description.","In case device name is Data Source, the path will be:","In case device name is Postgres Airlines demo database, the path will be:","Introduction: Briefly introduce the dataset and the source from where it was obtained. Adding fact and figures about the dataset makes it look more interesting. Describe how the data was collected. This could be through surveys, experiments, observations, or other methods. Mention the source of the dataset. This could be a link to the original dataset or the name of the organization that provided the data.","Note:","Recommended Usage","Recommended use in Python or R language: Mention the recommended usage of the dataset in Python or R language. This provides code to load file and preview.","Thing to consider while writing data sources path:","Things to consider while writing dataset description:","To access database tables in Python, use the following code:","To read data file in python use the following code:","Update frequency: Mention the frequency of the dataset update. This helps in understanding the dataset better. Update Frequency","Use H3 markdown format for writing headings","Use markdown format for writing dataset description"]},{"l":"Step 4: Uploading datasets to repositoy","p":["Dataset file of CSV format had to be uploaded to github repository Github Repository Dataset Repository","Dataset files must be stored in dataset folder only.","Configuration of datafiles must be done in config.csv file.","Syntax for configuration of datafiles in config.csv file: Adding Dataset to Repository","Syntax:","The dataset must be uploaded to the repository in the correct format and structure. This ensures that the dataset is easily accessible on datawars platform."]}],[{"i":"#","p":["Troubleshooting and resolving issues related to assertions in Python."]},{"l":"Assertions FAQ","p":["Assertions FAQ goes in this document."]}],[{"i":"#","p":["Resolving general issues."]},{"l":"FAQ: General Issues"},{"l":"1. What are the exact details we need to put in the invoice to receive payment?","p":["Solution: Below are the required details for the invoice:","Name","Legal Address","Zip Code","Phone Number","Account Number","SWIFT Code","IBAN Number"]},{"l":"2. What are the tools/techs we use at DataWars? Like GitHub, Docker.","p":["..."]}],[{"i":"#","p":["Troubleshooting and resolving issues related to GitHub Actions."]},{"l":"FAQ: GitHub Actions"},{"l":"1. Docker username and password are not present","p":["Error: Username and password required Solution: This action's error occurs when the GitHub repository does not have the username and password secrets configured.","To resolve this:","Create a new repository.","Move the content to the new repository.","Ask Anurag Verma to update the new repository in Airtable."]},{"l":"2. Import error: {success:false,error:'Skill Track'}","p":["{success:false,error:'Skill Track'} Solution: This error occurs when a skill track or skill is missing in Airtable.","To resolve this:","Contact Anurag Verma to add the missing skill track in the project on Airtable."]},{"l":"3. Import error: {success:false,error:No modules to import after filtering.}","p":["{success:false,error:No modules to import after filtering.} Solution: This occurs because the repository of this project is not added to Airtable.","To resolve this:","Contact Anurag or Santiago to add the repository to the Airtable record."]},{"l":"4. Platform Showing No Content","p":["No Content Solution: To display content on the platform:","Ensure that the GitHub Action completes at least once.","Wait for the actions to run fully and check back on the project."]}],[{"i":"#","p":["Solutions and best practices for common SQL project issues."]},{"l":"FAQ: SQL"},{"l":"1. What will be the template for the SQL project?","p":["Creating a new repository Solution: Use the SQL read-only template for SQL projects."]},{"l":"2. In the pandas project activity, we tested the assertions in the notebook. For the SQL project, what is the process to test the assertions?","p":["Solution:","Create a GitHub Repository: Follow the instructions here: Create a New Repo.","Update docker-compose.yml: Modify the docker-compose.yml file according to the databases you’re using (refer to the GitHub repository creation documentation linked above).","Share Your Repo: Once ready, share your repository with Anurag to update Airtable.","Build and Test Activities:","Start creating the activities.","Push activities and test them directly on the platform to ensure functionality."]},{"l":"3. I got this image path error while importing.","p":["Image Path Error Solution: The path of the images directory is incorrect. Use:","Correct: images/img-1.png","Incorrect: images\\img-1.png"]},{"l":"4. I got this database-related error.","p":["Invalid Database Error Solution: For SQL activities, ensure that the database device name in the code snippets matches the name in docker-compose.yml. Example:","Device name in code snippet. Device name in code snippet","Device name in docker-compose.yml: MySQL Device name in docker-compose.yml"]}],[{"l":"New authors training","p":["You will be working with a Github repository for your project. Let's understand how it works. This guide will help you understand the project structure, the artifacts in the repository, and how to transform the project, everything you need to know in order to start working on your project."]}],[{"l":"Your first project","p":["Getting Started with DataWars","What is an activity?","Activity Types","Choosing the dataset","Example notebook activities","Good Practices to follow by Authors"]}],[{"l":"Getting started","p":["It's important to understand the structure of a project first. The project is divided into two main sections:"]},{"l":"Left pane","p":["Contains the \"content\" of the project, including whatever the student has to read + the activities.","In the left pane, you can include activities, that are automatically graded. There are different activity types. Check What is an activity? and Activity Types for more."]},{"l":"Right pane","p":["Includes the \"interactive lab\", which is whatever is \"executable\" by the student. This should have almost NO content."]},{"l":"Next on","p":["Check the different What is an activity? and Activity Types available or how to create your first project from a Notebook."]}],[{"l":"What is an activity","p":["DataWars' projects are all about creating interesting and engaging activities. An activity in general contains:","Title: a short, descriptive, ideally imperative description of the objectives of the activity","Description: A free text (unlimited) description to help the student achieve the objectives of the activity, can include images or videos.","Solution(optional, strongly recommended): A solution that will be revealed upon request. Solutions can contain anything: text, code, images, GIFs, videos, etc. Although it's optional, MOST activities SHOULD contain a solution.","Hints(optional): A series of hints that can help the user. Also can contain any type of text, images, etc.","Expected outcome(optional): An example of what the outcome of the activity should be. It can be a screenshot of the dataframe, a table with the SQL results, an example of executing a function, etc.","There are different types of activities (read more in Activity Types):","Multiple choice","Input","Code/interactive with and without user input"]}],[{"l":"Activity types","p":["Here are the different activity types you can include in your project:"]},{"l":"Input activities","p":["Input activities give the user a blank field to enter their answer. It can be a numeric answer, or a text answer. For example:","How many null values are in the column X?: The answer is numeric, integer 250","What's the mean of the column X? The answer is numeric, but a float: 25.80.","What's the most common city in the dataset? The answer is a string New York City.","Float and string values are checked EXACTLY as they're provided. Strings are CASE SENSITIVE. And for floats, read the note below:","If your input activity's correct answer is a float, make sure you explain the user how many decimal values you're considering as valid, providing examples. Example:"]},{"l":"Multiple choice activities","p":["Multiple choice activities are as simple as they get. They can be configured to have one or more correct values (the widget rendered will be a radio button or a checkbox, respectively)."]},{"l":"Code activities (without input)","p":["\"Code\" activities are designed to check something from the user interactively. They're advanced types of activities and apply to MORE than just Jupyter activities. Internally, we can run custom code to check that the user's code passes the grading provided."]},{"l":"Code activities (with input)","p":["Similar to a regular code activity, the user has to provide some input that is evaluated dynamically."]}],[{"l":"How to create your first project","p":["Creating your first project is extremely simple. It all starts with a single Jupyter notebook. The general structure of your notebooks should be:","And here's the link to the notebook: Sample Project.ipynb"]}],[{"l":"Choosing a dataset","p":["Projects can be configured to use a DataWars Playgrounds data source automatically. Here's a step by step guide to do so.","In this document","How to use Playground Datasets in your projects","Examples of projects using datasources"]},{"l":"How to use Playground Datasets in your projects"},{"l":"1. Find the data source to use","p":["Data Sources can be either file-based datasets (a CSV, a directory containing image files) or a \"device\", for example, a MySQL Database.","Let's try to add both.","Go to the Datasets section of Playgrounds and find the datasets you want to add. In this case I'll add:","Pagila: a PostgreSQL database containing movie rentals","Hollywood Movies Database: a CSV file containing hollywood movies data","What you'll need to do is:","Go to datasets","Find the desired dataset (searching or using the filters on the right)","Hover over the logo and you'll see the ID","Copy the ID of the data source using the button","In this case, here are the IDs:","Pagila: 7742f868-902a-4262-bfbe-00497ac27468","Hollywood Movies Database: de5011a1-61fb-45f2-b067-bd363ba012d2"]},{"l":"2. Configure your Project's docker-compose.yml","p":["Now you have to define in your project's docker-compose.yml. There's a new section at the end of the YAML file that is called x-datasources:","You can list as many datasources as you want. If the datasource is a \"device\" type (like the Postgres Pagila one), it'll be automatically added to your project. But, if your dataset is a \"file type\" (a CSV, a Directory), you also have to configure it in the service that will have access to those files.","Here's a working example with comments:"]},{"l":"3. Finding the data in the lab","p":["If the data source is a \"device type\", it'll be in the final lab once you import it, easy.","If the data source is of file type, it'll be mounted in the /data directory."]},{"l":"Examples of working projects"},{"l":"Python + Pagila (postgres)","p":["This project ( Practice accessing Postres from Python using Pagila) has two devices:","Here's its docker-compose.yml:"]}],[{"i":"#","p":["Guidelines to create engaging and effective projects."]},{"l":"Good Practices for Project Authoring","p":["When designing a project, it’s crucial to keep two primary goals in mind:","Avoiding Boredom","Preventing Exhaustion","The following guidelines will help you meet these goals effectively."]},{"l":"1. Define and Manage Project Scope","p":["A well-defined scope is essential to ensure users face challenges that match their skill level and progress gradually. Understanding what the users already know and what they’re expected to learn is key.","For example, if you're creating a project within the Data Cleaning with Pandas skill track, asking users to perform a simple task like creating a Series from a column in a DataFrame may lead to boredom as it lacks sufficient challenge. Conversely, asking them to perform complex operations like merging datasets before covering the necessary concepts may overwhelm them. Ensure each project aligns with the skills already covered to keep users engaged without frustration."]},{"l":"2. Format Titles and Descriptions Thoughtfully","p":["Activity Titles: Always frame titles as questions to engage the user. For example, instead of “Capital of Argentina,” use “What is the Capital of Argentina?” This sparks curiosity and primes the user for problem-solving.","Activity Descriptions: Ensure that descriptions are clear, concise, and well-structured. Avoid long blocks of text, as they can overwhelm users, especially those juggling full-time work or study alongside their learning. Most users are looking to learn Data Science through hands-on coding exercises, so lengthy descriptions can exhaust them and make the project less enjoyable.","Use bold or italics to emphasize key points.","Enclose code elements (e.g., variable or column names) in backticks (`) to distinguish them from text.","Use triple backticks (```) for code blocks to improve readability.","If you can’t avoid lengthy descriptions, such as in capstone projects, try breaking the task into smaller activities. If that’s not feasible, use tools like ChatGPT to improve clarity and structure.","Example Prompt: You are a data science professor. I need your help improving the formatting and clarity of my activity descriptions. I'll provide the title and description, and you will revise them.","Example: Notice how ChatGPT transformed the activity title and description.","Before: Activity Before Formatting","After: Activity After Formatting","The Activities Should Be Atomic: One major drawback of longer activity descriptions is that they often lead to multiple steps within a single task. This complexity makes it harder to test and pinpoint errors. If an activity fails, users may not know where they made a mistake. For example, consider the following activity that requires users to complete four steps: calculating common_chatbots, defining the fill_chatbot function, applying this function to the chatbot_name column to impute missing values, and filling any remaining missing values with the string Unknown. If the task fails, users might struggle to identify the specific step where they made an error.","Not an Atomic Activity"]},{"l":"3. Use Placeholders in Jupyter Notebook","p":["Provide placeholders whenever users are asked to complete an activity. For example, if they need to store results in a variable or create a new column in a DataFrame, placeholders help guide them. This minimizes confusion and helps keep the task focused.","Examples of placeholders: Variable Placeholder Column Placeholder Function Placeholder"]},{"l":"4. Provide a Preview of Expected Outcomes","p":["Where possible, include a preview of the expected result. For example, in a visualization project, show the desired plot, or when manipulating a DataFrame, provide a snapshot of the expected output. These visual or textual cues help users understand what the end goal looks like, reducing frustration and ensuring they are on the right path.","Examples: Expected Outcome DataFrame Expected Outcome Visualization"]},{"l":"5. Practice Empathy","p":["Think from the user’s perspective and ask:","What might confuse or challenge the user at this stage?","Are the instructions clear for someone with the expected level of knowledge?","Would I feel confident completing this task, given what I’ve learned so far?","Empathy-driven design leads to a smoother learning experience by anticipating user needs.","Example: Example Activity","Consider the activity in the screenshot above where users are asked to input answers. If they can't solve it and request a solution, they receive the correct value to pass. But pause and think—what was on their minds?","The users requested help because:","They wrote code but got the wrong result, or","They didn’t know how to approach the problem.","In both cases, just giving the answers isn’t enough. They expect to learn how to solve the problem. The solution should include code or explanations demonstrating the correct process, reinforcing their learning for future tasks."]},{"l":"6. Anticipate and Address Edge Cases","p":["Be mindful of edge cases and provide detailed instructions to address them. Whenever an edge case arises, clearly outline the steps users should follow to handle it. This prevents users from feeling stuck or confused when encountering unexpected situations.","For example: Look how different ways of calculating frequency counts result in different order of index when their values are equal. In this case, we should mention in the activity description that the user should use value_counts().","Edge Case Example"]},{"l":"7. Manage Time Expectations and Feedback","p":["Consider the time required to complete the project. For Learn and Practice projects, aim for 25–30 minutes, while for Assignments and Capstone Projects, a duration of 45–60 minutes is ideal. Managing time expectations ensures the project remains engaging and manageable without overwhelming users.","By following these best practices, you’ll create projects that are engaging, appropriately challenging, and enjoyable for learners. This balance will help keep users motivated and make their learning experience both rewarding and frustration-free."]}],[{"i":"#","p":["Guidelines to create Knowledge Test."]},{"l":"Knowledge Tests","p":["Knowledge tests are designed to gauge a user's understanding of fundamental, technical, and practical knowledge within a given skill."]},{"l":"Structure of a Knowledge Test","p":["Each knowledge test consists of three types of multiple-choice questions:"]},{"l":"1. Conceptual Questions","p":["These questions assess the user's understanding of the core concepts of a skill. They are language-agnostic and focus on theoretical knowledge.","Example:"]},{"l":"2. Syntactical Questions","p":["These questions test the user's knowledge of syntax used in coding. They are specific to programming languages and help evaluate familiarity with code structure and rules.","Example:"]},{"l":"3. Scenario-Based Questions","p":["These questions present a real-world scenario with associated dummy data. They simulate practical applications of the skill, requiring users to apply their knowledge in realistic situations.","Example:"]},{"l":"Guidelines for Creating Questions","p":["When designing knowledge test questions, follow these rules:","One or more options may be correct.","Each question must be relevant to the skill being tested."]},{"l":"Example Questions & Structure","p":["Examples of different question types and the english.md structure can be found in this repository: Intro to Pandas for Data Analysis - Knowledge Test"]},{"l":"Structure of english.md","p":["Each question type is documented on a separate page, with relevant activities written within those pages. Below is the general structure:"]}],[{"i":"#","p":["This checklist outlines the criteria that must be met for a project to be considered complete. It ensures that all necessary activities are finished and all required files are present and accounted for."]},{"l":"Project Completion Checklist"},{"l":"Project: Ready to Publish? Verify with This Checklist","p":["This checklist outlines the criteria that must be met for a project to be considered complete. It ensures that all activities are finished and all required files are present and accounted for.","Regarding activities:","Detailed official solutions are available.","Passed the GPT Test.","Passed when tested with the official solutions.","Are atomic; an activity should not ask multiple things at once.","General Project Checks:","The author has finished the project on the platform.","The difficulty level has been mentioned.","Expected outcome must be present wherever necessary.","Required files are present:","chat.txt(for the DataWars AI Assistant)","public.md(short, eye-catching description of the project)"]}],[{"l":"Working with Github","p":["Projects at DataWars are configured with a Github repository. Here's the overview of how it works:","Creating a github account","Creating a new repository","Project Structure","Reference: All artifacts in a Github repo","Automatic build/import actions","Transforming the project"]}],[{"l":"Reference of Repo most important components","p":["Lab Content","english.md","chat.txt","public.md","docker-compose.yml"]}],[{"l":"Lab contents","p":["The left hand side of a project is the result of the english.md file. The right hand side, \"the lab\", is the result of the Docker image built during the project creation phase.","The process is advanced and you have a lot of control about how to build your image. But, for now, the only thing you have to know is that ANYTHING that you include in the notebooks/ directory will be included in the final lab image.","With the exception of Solution.ipynb and activity_solutions_files that have a special treatment."]},{"l":"Notebooks","p":["The main notebook you have to worry about for now is Project.ipynb, which is the \"stripped down/clean\" version of your `Solution.ipynb\"."]},{"l":"Adding your datasets","p":["Just include your datasets in the notebooks/ directory. I'd recommend just leaving them in the root of the folder. So, for example, upload the dataset to notebooks/pokemon.csv. The Project.ipynb notebook will then read it as pd.read_csv('pokemon.csv'), as Project.ipynb and pokemon.csv are in the same directory:","If you want, you can create a subdirectory, but this is not necessary (actually discouraged for simple datasets) for example:","In this case, the notebook will read the file as: pd.read_csv(data/pokemon.csv)."]},{"l":"Special libraries and requirements","p":["If your project uses any libraries outside of the regular stack from DataWars projects, you can include them in the requirements.txt"]},{"l":"Advanced usage","p":["This is only intended for advanced projects/users. You probably don't need it.","The Dockerfile contains the steps used to build the lab. You can fully customize this if required. The docker-compose.yml can contain special values:"]}],[{"l":"english.md","p":["The english.md file defines the content on the left panel of a project. It's a markdown file with special HTML tags defined by us. The main tags you have to know are page and activity. All tags must contain a UNIQUE ID (unique within the project).","You don't have to manually code the tags, as we have VSCode snippets that will make your life easier.","Download Snippets"]},{"l":"Pages","p":["Pages are MANDATORY and where all the other elements live. You can have as many pages as you want and they define the big \"sections\" on the left side panel. They can contain any markdown text or images you want to add as well as activities.","Pages are defined with the syntax:"]},{"l":"Activities","p":["Activities live within a page component. Depending on the activity type, they'll support special attributes and internal tags. But they all have a title attribute, and they support any text as a description:","As you can see from above, the activity description accepts any valid markdown code, including images.","All activities must contain the hint and solution tags. But you can leave them empty if you don't want to provide a hint/solution, as these are optional (although strongly recommended).","Specific activity types:"]},{"l":"Input activities","p":["Input activities ask the user for their answer by rendering a simple input widget. Here's the syntax:","When the user submits their answer, we check if the answer matches whatever you specified in correct-answer."]},{"l":"Multiple choice activities","p":["Here's the syntax of a multiple choice activity:","The only important thing to know is that the widget attribute will change the activity to accept only one correct answer (rendering HTML radio buttons) to multiple correct answers (rendering checkboxes). The accepted values are radio or checkbox respectively. Here's an example of a checkbox activity:"]},{"l":"Code Activities","p":["These are probably the most important and \"complicated\" activities. There are a lot details that can be configured for a code activity. But for now, we'll keep it simple and we'll document only a standard Jupyter activity.","You can ignore for now the attributes type, template and device as well as the metadata tag at the end. The only important tag for you to know is the validation-code one, which includes your assertions.","Never use the angle brackets and in the markdown content. They are used here to indicate the tags."]}],[{"l":"chat.txt","p":["The chat.txt file contains the context that will be passed to the AI assistant whenever the student asks a question. It should have the following structure:","The learning objectives are related to the skill your project covers. You can be more specific providing more details. For example, if you're working on Filtering and Sorting with Pandas, but your project heavily relies on boolean operators, you can be more specific and add:","The description of the data should be the characteristics of the data the student is dealing with. Be AS BRIEF as possible, as this counts towards the total TOKEN Count and is MORE EXPENSIVE. For example:","If your data is a tabular file (a CSV, dataframe), you can use the df.info() methods and just print the df.head().to_markdown().","If your data is a database, you can just tell the agent that \"the student is working with the MySQL Sakila sample database\".","Below is the example of a chat.txt file:","chat-txt","This example has 3 parts:","The objectives of the project.","The data the student has. (2-3 liner intro + first 10 rows of the data)","The info of the dataset.( df.info())"]}],[{"l":"public.md","p":["The public.md file contains the public description of a project in markdown format. Max length is 500 characters."]}],[{"l":"docker-compose.yml","p":["This is a special file that won't be needed for now, only in exceptional cases. Leave it unchanged."]}],[{"l":"generate_solutions.py","p":["Hi everyone! I wanted to share a finding with you regarding our ongoing update of the base images for our labs (including updating pandas, numpy, matplotlib, and other packages). We've encountered an issue related to the activity solution files, particularly with Matplotlib, though it could apply to other files as well.","For instance: If we create a solution image using Matplotlib version 3.9.0 and later update the lab with the latest version, 3.10.0, we face a problem. When comparing the solution image we created (with version 3.9.0) to a user’s solution image (which uses version 3.10.0), the images will look identical, but the user won't pass the assertion due to version differences."]},{"l":"Possible Solution:","p":["We’re considering generating dynamic solution files. Each time we push changes to the project on GitHub, the solution files will be automatically regenerated. This way, the solution files will always match the latest version of the packages in the lab.","Here’s what we have done in this repo: https://github.com/datawars-io-content/lab-843g8gf-plotting-basics-hotel-rating-trip-types","New generate_solutions.py file: We have added a new file in the notebooks/ directory called generate_solutions.py. This file contains all the code from the Solution.ipynb notebook and generates the activity solution files. The code must contain same activity solution file names that are used in english.md. Location of generate_solutions.py","Dockerfile Update: In the notebooks/ directory, We have updated the Dockerfile to include: RUN [ -f generate_solutions.py ] python generate_solutions.py This ensures that new solution files are created every time we push an update to the GitHub repo. Dockerfile contents"]}],[{"l":"Getting started with Github","p":["TODO","Create an account","Understand the basics of git"]}],[{"l":"Creating a new repository for your project","p":["Check the .yml file in the repository and change the docker-compose.yml file according to the database you want to use.","Check this document on how to find dataset from datasources playground: Finding Datasets from Datasources Playground","Create a new repository using the + sign.","Data path: Mention the path of the dataset. This is important as it helps in locating the dataset easily.","Data Source Path","Depending on the project, you can choose the template. For the SQL project, you can choose the SQL template.","Depending on your project, you can choose jupyter template or the sql template.","Example of docker-compose.yml: Querying Logical Operator with Airlines using Python","Example of english.md: Querying Logical Operator with Airlines using Python","Example: lab-1234-credit-card-fraud-detection","For data files with CSV file format, the path can be mentioned as follows: {{datasource.filesystem_path}}/\"filename\"","For databases, the ip address path can be written as follows: {{devices_copy.\"Data Source\".ip_address}} Dataset path","For Example:","For your SQL projects, you have to setup the docker-compose.yml file. You can refer to the Available Databases for the list of databases available.","Format of the repository name: lab-{random ID}-{project_name}","Here is the explample:","Here is the path to get ip address of devices rendered in description.","In case device name is Data Source, the path will be:","In case device name is Postgres Airlines demo database, the path will be:","Make sure do not put any characters before lab- and after the project name.","new-repo","new-repo-button","Now, click on the Create repository button.","Random ID can be of any number and alphabets except special characters without any spaces.","Select the template","select-template","The project name should be in small case separated by - or _(dash or underscore).","Thing to consider while writing data sources path:","This is how your repository looks.","When you select the dataset from the datasources playground, you can get the name of the database and ip address of the database from the docker-compose.yml file and you can include the same in the english.md file.","Write the repository name","write-repo-name"]}],[{"l":"Github project structure","p":["This page contains a summary of each of the most important components in a project. The reference contains detailed information of each one of them."]},{"l":"1. english.md","p":["The MOST important file. It contains all the content that is rendered on the left side of the screen. It contains all the text of the pages and the activities.","Refer to the main documentation about english.md for a full description including the snippets to download for VS Code.","High level overview of the structure:","english.md","notebooks/","public.md","chat.txt","docker-compose.yml"]},{"l":"2. public.md","p":["This markdown file contains the public description of the project. Read more."]},{"l":"3. chat.txt","p":["The context for the AI Assistant, including objectives of the project and description of the data. Read more."]},{"l":"4. Notebooks and lab files","p":["The lab (right hand size) of your project will contains the notebooks, datasets, images, and other components needed. You can install custom requirements."]}],[{"l":"Automatic build/import with actions","p":["Once you push your project, the Github actions configured will be triggered automatically validating your project, building the docker image and importing the project into the DataWars platform."]}],[{"l":"Transforming your project","p":["Remember to clear all the outputs of your Project.ipynb notebook before pushing."]}],[{"l":"Instruction on where and which channels to use for communication/asking queries/doubs, etc."}],[{"l":"Configuring ai tools","p":["We use three main AI helpers at DataWars:","continue.dev for VSCode","sgpt for command line help","jupyter-ai","Private API Keys to setup AI are located in the following document","Don't share the API Keys with ANYBODY. This is highly sensitive information."]},{"l":"Setting up continue.dev","p":["If you have any doubts, go to the Continue docs","Here's a quick setup guide:","For the most part, you can use the gpt-4o-mini, as it's a fast, effective and cheap model. For more advanced tasks you can use gpt-4o, but it's more expensive. Use it with discretion.","The config file for continue.dev will look something like:"]},{"l":"SGPT","p":["SGPT docs: https://github.com/TheR1D/shell_gpt","SGPT is very simple to install, just pip install shell-gpt.","Here's a quick start guide:"]}],[{"l":"Getting paid","p":["To get paid, you need to send an invoice of the previous month to ap@datawars.io. The invoice has to be in US Dollars and contain your full name and instructions to get paid (your bank account details)."]},{"l":"Example:","p":["The invoice should contain:","For Indian team members, the Intstuctions to get paid must include SWIFT Code, Account Number and IFSC Code."]},{"l":"Proportional invoices","p":["If you started on an odd date that is not the beginning of the month, you must send a proportional invoice for your services. So for example, if you started on September 15th, and your salary is $100/month, you should send on the first day of October an invoice for $100/30 * 15. Next month, you'll send a full invoice and go back to schedule."]}],[{"l":"Assessment Creation Documentation","p":["Welcome to the comprehensive guide for creating assessments within the DataWars platform. This document serves as your primary resource for understanding the components, best practices, and implementation details essential for developing effective skill assessments."]},{"l":"Platform Architecture"},{"l":"\uD83C\uDFD7️ Learning Hierarchy","p":["The DataWars platform implements a structured hierarchy that organizes content from broad career paths to specific learning objectives. Each level serves a distinct purpose in the learning journey:","Career Path:","DataWars organizes learning content into career-focused paths. The Data Analysis career path, for instance, provides a comprehensive learning journey for aspiring data analysts and scientists.","Skill Track:","Within each career path, skill tracks focus on specific technologies or methodologies. The \"Intro to Pandas\" skill track, for example, provides a foundational understanding of the Pandas library for data manipulation.","Skills:","Each skill track contains multiple skills that build upon each other. Within the \"Intro to Pandas\" track, \"Pandas Series Basics\" represents a fundamental skill that learners must master before progressing to more advanced concepts.","Objectives:","Skills are further broken down into specific, measurable objectives. For the \"Pandas Series Basics\" skill, objectives include mastering series creation, understanding indexing operations, and implementing various series methods."]},{"l":"Example"},{"l":"Assessment Development"},{"l":"\uD83D\uDCCA Assessment vs. Project Distinction","p":["In the context of Pandas Series Basics, the distinction between projects and assessments becomes clear:","Coverage","A project might focus on series creation and basic operations","An assessment evaluates all three objectives: creation, indexing, and methods","New Requirements","Assessment configuration specifies all three objectives","Pages map directly to these fundamental Pandas operations","Activities test comprehensive series manipulation skills"]},{"l":"\uD83D\uDEE0️ Core Components"},{"l":"1. Assessment Configuration Structure","p":["The assessment.json file requires specific configuration fields that define the assessment behavior:","Required Fields:","scoring_config: Determines the scoring methodology","Option 1: Per-objective scoring","Option 2: Global scoring","objectives: List of testable objectives","expiration_time: Optional time limit in minutes","Basic Structure Example:","Implementation Example with Repository-Specific IDs:"]},{"l":"\uD83D\uDCDD Page Attributes in english.md","p":["The english.md file implements specific page attributes that control assessment behavior and objective mapping. These attributes define how content is presented to users and ensure proper evaluation of skills.","Required Attribute:","objective: Maps the page content to specific objectives defined in assessment.json","Format: objective=tr54a-objective-11","Example: For a page covering Selection and Indexing, use the corresponding objective ID","Optional Attributes:","shuffle-activities: Randomizes the presentation order of activities within a page","Different users see activities in different sequences","Helps prevent pattern recognition and memorization","Format: Add shuffle-activities to page tag","random-sample-amount: Controls the number of activities shown to each user","Specifies maximum activities displayed","Format: random-sample-amount=N where N is the desired number","Example: random-sample-amount=5 shows 5 random activities from the available pool","Example Implementation:"]},{"l":"english.md Content Implementation"},{"l":"Implementation Example"},{"l":"\uD83D\uDCC2 Basic Repository Structure","p":["Using our Pandas Series project (lab-tr54a-badge-project-8):"]},{"l":"Objective ID Convention","p":["The repository name influences the objective IDs:","Repository: lab-tr54a-badge-project-8","Objective IDs: tr54a-objective-11 (Series Creation)","Page references: objective=tr54a-objective-11"]},{"l":"Frequently Asked Questions","p":["Do we need chat.txt file?","No, chat.txt is not required for assessments.","How do we handle activities in Project.ipynb?","Activity numbers and titles are not needed in Project.ipynb. The notebook should maintain its normal flow without specific activity markers.","Will the docker-compose file configuration change?","No, use the same docker-compose configuration as existing projects.","Should we transform existing projects into assessments?","No, create new projects instead of modifying existing ones. Each assessment should have its own new repository.","How should we handle activity sampling and shuffling during beta testing?","During beta testing phase, do not implement any type of sampling or shuffling functionalities.","How do repository names relate to objective IDs?","Extract prefix from repository name (e.g., \"tr54a\" from lab-tr54a-badge-project-8)","Use format: prefix-objective-number (e.g., tr54a-objective-11)"]}]]